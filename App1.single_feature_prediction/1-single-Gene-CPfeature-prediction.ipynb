{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "# import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import pairwise_distances,mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from utils.readProfiles import *\n",
    "from utils.pred_models import *\n",
    "from utils.saveAsNewSheetToExistingFile import saveAsNewSheetToExistingFile\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "# from utils import networksEvol, tsne, readProfiles\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "- CDRP-BBBC047-Bray-CP-GE (Cell line: U2OS):\n",
    "    * There are 30,430 and 21,782 unique compounds for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: 4 (approx), GE: 3 (approx). \n",
    "    * 20,358 compounds are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (153386 , 1783) ,  ge:  (68120 , 977)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp:  (30430, 1786)   ,  ge:  (21782, 981) \n",
    "    * Merged Profiles Shape:                              (20358, 2766)\n",
    "    * High Rep corr: CP: 30618 to 7892, l1k: 21069 to 2870, overlap:  3\n",
    "    \n",
    "    \n",
    "- CDRPBIO-BBBC036-Bray-CP-GE (Cell line: U2OS):\n",
    "    * There are 30,430 and 21,782 unique compounds for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: 4 (approx), GE: 3 (approx). \n",
    "    * 20,358 compounds are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (153386 , 1783) ,  ge:  (68120 , 977)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp:  (30430, 1786)   ,  ge:  (21782, 981) \n",
    "    * Merged Profiles Shape:                   \n",
    "    * High Rep corr: CP: 2239 to 312, l1k: 1535 to 448, overlap:  131\n",
    "\n",
    "\n",
    "- LUAD-BBBC041-Caicedo-CP-GE (Cell line: A549) : \n",
    "    * There are 593 and 529 unique alleles for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: 8 (approx), GE: 8 (approx).\n",
    "    * 525 alleles are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (6144 , 1783) ,  ge:  (4232 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp: (593, 1786) , ge: (529, 979) \n",
    "    * Merged Profiles Shape:                             (525, 2764)    \n",
    "    * High Rep corr: CP: x to x, l1k: x to x, overlap:  x    \n",
    "    \n",
    "    \n",
    "- TA-ORF-BBBC037-Rohban-CP-GE (Cell line: U2OS) :\n",
    "    * There are 299 and 226 number of unique compounds for CP and GE datasets respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: 5 (approx), GE: 2 (approx).\n",
    "    * 188 alleles are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures):         cp: (1920 , 1783) ,  ge: (729 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures+metadata):cp: (323, 1784)   ,  ge: (328, 979)\n",
    "    * Merged Profiles Shape:                                     (149, 2762)\n",
    "    * High Rep corr: CP: x to x, l1k: x to x, overlap:  x    \n",
    "    \n",
    "    \n",
    "- LINCS-Pilot1-CP-GE (Cell line: A549) :\n",
    "    * There are 1570 unique compounds across 7 doses for CP dataset\n",
    "    * There are x unique compounds across 7 doses for GE dataset\n",
    "    * Median number of replicates for each dataset is as follows: CP: 5 (approx), GE: 3 (approx).\n",
    "    * 6984 \"compounds-dose\" are present in both datasets. \n",
    "    * Replicate Level Shapes (nSamples x nFeatures):         cp: (52223 , 1747) ,  ge: (27837 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures+metadata):cp: (9394, 1748)   ,  ge: (8370, 979)\n",
    "    * Merged Profiles Shape:                                     (6984, 2726)\n",
    "    * High Rep corr: CP: 9394 to 4647, l1k: 8369  to  2338, overlap:  1140\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procProf_dir='/home/ubuntu/datasetsbucket/Rosetta-GE-CP/'\n",
    "# procProf_dir='/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/'\n",
    "procProf_dir = \"../../../rosetta/broad/workspace/\"\n",
    "# metadata_dir='/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/metadata/'\n",
    "results_dir = \"./results/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of single GE expression levels based on the full CP profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "datasets = [\"LUAD\"]\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "# datasets=['TAORF','LUAD','LINCS', 'CDRP-bio']\n",
    "\n",
    "DT_kfold = {\"LUAD\": 10, \"TAORF\": 5, \"LINCS\": 25, \"CDRP-bio\": 6, \"CDRP\": 40}\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType = \"normalized_variable_selected\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"highRepOverlap\"\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "\n",
    "\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "# def f(dataset):\n",
    "for dataset in datasets:\n",
    "\n",
    "    (\n",
    "        mergProf_treatLevel,\n",
    "        cp_features,\n",
    "        l1k_features,\n",
    "    ) = read_paired_treatment_level_profiles(\n",
    "        procProf_dir, dataset, profileType, filter_perts, 1\n",
    "    )\n",
    "\n",
    "    l1k = mergProf_treatLevel[[pertColName] + l1k_features]\n",
    "    cp = mergProf_treatLevel[[pertColName] + cp_features]\n",
    "\n",
    "    if dataset == \"LINCS\":\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"].str[0:13]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"].str[0:13]\n",
    "    else:\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels = le.fit_transform(l1k[\"Compounds\"].values)\n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled = l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values)\n",
    "    cp_scaled = cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(\n",
    "        cp[cp_features].values.astype(\"float64\")\n",
    "    )\n",
    "\n",
    "    for model in [\"Lasso\"]:  # [\"Lasso\",\"MLP\",\"RFR\"]:\n",
    "\n",
    "        if 1:\n",
    "            cp = cp_scaled.copy()\n",
    "            l1k = l1k_scaled.copy()\n",
    "\n",
    "        ##############################\n",
    "\n",
    "        k_fold = DT_kfold[dataset]\n",
    "\n",
    "        pred_df = pd.DataFrame(index=range(k_fold), columns=l1k_features)\n",
    "        pred_df_rand = pd.DataFrame(index=range(k_fold), columns=l1k_features)\n",
    "        ii = 0\n",
    "        for l in l1k_features:\n",
    "            ii += 1\n",
    "            print(ii)\n",
    "            if model == \"Lasso\":\n",
    "                scores, scores_rand = lasso_cv_plus_model_selection(\n",
    "                    cp[cp_features], l1k[l], k_fold, group_labels, 1\n",
    "                )\n",
    "            #                 sfaadadd\n",
    "            #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "            elif model == \"MLP\":\n",
    "                scores, scores_rand = MLP_cv_plus_model_selection(\n",
    "                    cp[cp_features], l1k[l], k_fold, group_labels, 1\n",
    "                )\n",
    "            #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "            elif model == \"Ridge\":\n",
    "                scores, scores_rand = ridge_cv_plus_model_selection(\n",
    "                    cp[cp_features], l1k[l], k_fold, group_labels, 1\n",
    "                )\n",
    "            #                 gafgfdssgfd\n",
    "            #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "            elif model == \"RFR\":\n",
    "                scores, scores_rand = RFR_cv_plus_model_selection(\n",
    "                    cp[cp_features], l1k[l], k_fold, group_labels, 1\n",
    "                )\n",
    "\n",
    "            pred_df[l] = scores\n",
    "            pred_df_rand[l] = scores_rand\n",
    "\n",
    "        ########################### mapping prob_ids to genes names\n",
    "        pred_df, _ = rename_affyprobe_to_genename(pred_df, l1k_features)\n",
    "        pred_df_rand, _ = rename_affyprobe_to_genename(pred_df_rand, l1k_features)\n",
    "\n",
    "        meltedPredDF = pd.melt(pred_df).rename(\n",
    "            columns={\"variable\": \"lmGens\", \"value\": \"pred score\"}\n",
    "        )\n",
    "        meltedPredDF_rand = pd.melt(pred_df_rand).rename(\n",
    "            columns={\"variable\": \"lmGens\", \"value\": \"pred score\"}\n",
    "        )\n",
    "        meltedPredDF[\"d\"] = \"n-folds\"\n",
    "        meltedPredDF_rand[\"d\"] = \"random\"\n",
    "        filename = results_dir + \"/SingleGenePred/scores.xlsx\"\n",
    "\n",
    "        profTypeAbbrev = \"\".join([s[0] for s in profileType.split(\"_\")])\n",
    "\n",
    "#         saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "#                                      model+'-'+dataset+'-'+profTypeAbbrev+'-'+f+'-'+str(k_fold)+'-ht')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1k_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of single CP features based on the full GE profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "datasets = [\"LINCS\", \"CDRP-bio\"]\n",
    "# datasets=['TAORF','CDRP'];\n",
    "\n",
    "DT_kfold = {\"LUAD\": 10, \"TAORF\": 10, \"LINCS\": 20, \"CDRP-bio\": 10, \"CDRP\": 40}\n",
    "\n",
    "# DT_kfold={'LUAD':9, 'TAORF':5, 'LINCS':25, 'CDRP-bio':6,'CDRP':40}\n",
    "\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "# 'normalized_feature_select_dmso'\n",
    "profileType = \"normalized\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"highRepOverlap\"\n",
    "\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "# def f(dataset):\n",
    "for dataset in datasets:\n",
    "\n",
    "    (\n",
    "        mergProf_treatLevel,\n",
    "        cp_features,\n",
    "        l1k_features,\n",
    "    ) = read_paired_treatment_level_profiles(\n",
    "        procProf_dir, dataset, profileType, filter_perts, 1\n",
    "    )\n",
    "\n",
    "    l1k = mergProf_treatLevel[[pertColName] + l1k_features]\n",
    "    cp = mergProf_treatLevel[[pertColName] + cp_features]\n",
    "\n",
    "    if dataset == \"LINCS\":\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"].str[0:13]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"].str[0:13]\n",
    "    else:\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels = le.fit_transform(l1k[\"Compounds\"].values)\n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled = l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values)\n",
    "    cp_scaled = cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(\n",
    "        cp[cp_features].values.astype(\"float64\")\n",
    "    )\n",
    "\n",
    "    for model in [\"Lasso\", \"MLP\"]:\n",
    "\n",
    "        #         if model==\"MLP\":\n",
    "        #             cp_scaled[cp_features] =preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit_transform(cp_scaled[cp_features].values)\n",
    "        #             cp_scaled[l1k_features] =preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit_transform(cp_scaled[l1k_features].values)\n",
    "\n",
    "        if 1:\n",
    "            cp = cp_scaled.copy()\n",
    "            l1k = l1k_scaled.copy()\n",
    "\n",
    "        ##############################\n",
    "\n",
    "        k_fold = DT_kfold[dataset]\n",
    "        #         k_fold=int(np.unique(group_labels).shape[0]/20)\n",
    "        pred_df = pd.DataFrame(index=range(k_fold), columns=cp_features)\n",
    "        pred_df_rand = pd.DataFrame(index=range(k_fold), columns=cp_features)\n",
    "        ii = 0\n",
    "        for c in cp_features:\n",
    "            ii += 1\n",
    "            print(ii)\n",
    "            if model == \"Lasso\":\n",
    "                scores, scores_rand = lasso_cv_plus_model_selection(\n",
    "                    l1k[l1k_features], cp[c], k_fold, group_labels, 1\n",
    "                )\n",
    "            elif model == \"MLP\":\n",
    "                scores, scores_rand = MLP_cv_plus_model_selection(\n",
    "                    l1k[l1k_features], cp[c], k_fold, group_labels, 1\n",
    "                )\n",
    "            pred_df[c] = scores\n",
    "            pred_df_rand[c] = scores_rand\n",
    "\n",
    "        meltedPredDF = pd.melt(pred_df).rename(\n",
    "            columns={\"variable\": \"CP-Features\", \"value\": \"pred score\"}\n",
    "        )\n",
    "        meltedPredDF_rand = pd.melt(pred_df_rand).rename(\n",
    "            columns={\"variable\": \"CP-Features\", \"value\": \"pred score\"}\n",
    "        )\n",
    "        meltedPredDF[\"d\"] = \"n-folds\"\n",
    "        meltedPredDF_rand[\"d\"] = \"random\"\n",
    "        #         filename='../../results/SingleCPfeatPred/scores.xlsx'\n",
    "        filename = results_dir + \"/SingleCPfeatPred/scores.xlsx\"\n",
    "\n",
    "        profTypeAbbrev = \"\".join([s[0] for s in profileType.split(\"_\")])\n",
    "\n",
    "        saveAsNewSheetToExistingFile(\n",
    "            filename,\n",
    "            pd.concat([meltedPredDF, meltedPredDF_rand], ignore_index=True),\n",
    "            model\n",
    "            + \"-\"\n",
    "            + dataset\n",
    "            + \"-\"\n",
    "            + profTypeAbbrev\n",
    "            + \"-\"\n",
    "            + f\n",
    "            + \"-\"\n",
    "            + str(k_fold)\n",
    "            + \"-ht\",\n",
    "        )\n",
    "\n",
    "\n",
    "#     return\n",
    "\n",
    "# with Pool(10) as p:\n",
    "#     p.map(f, datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CP Category specific scores for single gene prediction - based on Lasso  regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning(\"ignore\")\n",
    "\n",
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "# datasets=['LUAD', 'TAORF', 'LINCS', 'CDRP-bio'];\n",
    "datasets = [\"LUAD\"]\n",
    "\n",
    "# DT_kfold={'LUAD':10, 'TAORF':5, 'LINCS':20, 'CDRP-bio':20,'CDRP':40}\n",
    "\n",
    "DT_kfold = {\"LUAD\": 9, \"TAORF\": 5, \"LINCS\": 25, \"CDRP-bio\": 6, \"CDRP\": 40}\n",
    "\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "# 'normalized_feature_select_dmso'\n",
    "profileType = \"normalized\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"highRepOverlap\"\n",
    "\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "profileLevel = \"treatment\"\n",
    "#'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "# def f(dataset):\n",
    "for dataset in datasets:\n",
    "\n",
    "    (\n",
    "        mergProf_treatLevel,\n",
    "        cp_features,\n",
    "        l1k_features,\n",
    "    ) = read_paired_treatment_level_profiles(\n",
    "        procProf_dir, dataset, profileType, filter_perts, 1\n",
    "    )\n",
    "\n",
    "    l1k = mergProf_treatLevel[[pertColName] + l1k_features]\n",
    "    cp = mergProf_treatLevel[[pertColName] + cp_features]\n",
    "\n",
    "    if dataset == \"LINCS\":\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"].str[0:13]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"].str[0:13]\n",
    "    else:\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels = le.fit_transform(l1k[\"Compounds\"].values)\n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled = l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values)\n",
    "    cp_scaled = cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(\n",
    "        cp[cp_features].values.astype(\"float64\")\n",
    "    )\n",
    "\n",
    "    if 1:\n",
    "        cp = cp_scaled.copy()\n",
    "        l1k = l1k_scaled.copy()\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    # k_fold=10\n",
    "    k_fold = DT_kfold[dataset]\n",
    "    Channelss = [\"DNA\", \"RNA\", \"AGP\", \"Mito\", \"ER\"]\n",
    "    featureGroups = [\"Texture\", \"Intensity\", \"RadialDistribution\"]\n",
    "    relationMat_mpCat = pd.DataFrame(index=l1k_features)\n",
    "    for ch in range(len(Channelss)):\n",
    "        for f in range(len(featureGroups)):\n",
    "            print(ch, f)\n",
    "            selectedCols = cp.columns[\n",
    "                cp.columns.str.contains(Channelss[ch])\n",
    "                & cp.columns.str.contains(featureGroups[f])\n",
    "                & cp.columns.str.contains(\"Cells_|Cytoplasm_|Nuclei_\")\n",
    "            ].tolist()\n",
    "            if selectedCols != []:\n",
    "                for l in l1k_features:\n",
    "                    #                     scores,scores_rand=lasso_cv_plus_model_selection(cp[selectedCols],l1k[l],k_fold,group_labels,0)\n",
    "                    scores, scores_rand = MLP_cv_plus_model_selection(\n",
    "                        cp[selectedCols], l1k[l], k_fold, group_labels, 0\n",
    "                    )\n",
    "\n",
    "                    relationMat_mpCat.loc[\n",
    "                        l, Channelss[ch] + \"_\" + featureGroups[f]\n",
    "                    ] = np.median(scores)\n",
    "\n",
    "    Channelss = [\"Nuclei\", \"Cytoplasm\", \"Cells\"]\n",
    "    featureGroups = [\"AreaShape\"]\n",
    "\n",
    "    for ch in range(len(Channelss)):\n",
    "        for f in range(len(featureGroups)):\n",
    "            selectedCols = cp.columns[\n",
    "                cp.columns.str.contains(Channelss[ch])\n",
    "                & cp.columns.str.contains(featureGroups[f])\n",
    "                & cp.columns.str.contains(\"Cells_|Cytoplasm_|Nuclei_\")\n",
    "            ].tolist()\n",
    "            if selectedCols != []:\n",
    "                for l in l1k_features:\n",
    "                    #                     scores,scores_rand=lasso_cv_plus_model_selection(cp[selectedCols],l1k[l],k_fold,group_labels,0)\n",
    "                    scores, scores_rand = MLP_cv_plus_model_selection(\n",
    "                        cp[selectedCols], l1k[l], k_fold, group_labels, 0\n",
    "                    )\n",
    "\n",
    "                    relationMat_mpCat.loc[\n",
    "                        l, Channelss[ch] + \"_\" + featureGroups[f]\n",
    "                    ] = np.median(scores)\n",
    "\n",
    "    ########################### mapping prob_ids to genes names\n",
    "    meta = pd.read_csv(\n",
    "        \"../affy_probe_gene_mapping.txt\",\n",
    "        delimiter=\"\\t\",\n",
    "        header=None,\n",
    "        names=[\"probe_id\", \"gene\"],\n",
    "    )\n",
    "    meta_gene_probID = meta.set_index(\"probe_id\")\n",
    "    d = dict(zip(meta_gene_probID.index, meta_gene_probID[\"gene\"]))\n",
    "    relationMat_mpCat = relationMat_mpCat.rename(index=d)\n",
    "\n",
    "    filename = results_dir + \"/SingleGenePred_cpCategoryMap/cat_scores_maps.xlsx\"\n",
    "\n",
    "    profTypeAbbrev = \"\".join([s[0] for s in profileType.split(\"_\")])\n",
    "\n",
    "    saveAsNewSheetToExistingFile(\n",
    "        filename, relationMat_mpCat, dataset + \"-\" + str(k_fold) + \"-MLP-ht\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp.columns[cp.columns.str.contains('RadialDistribution')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topGenes_mito_radial.sort_values(by='Mito_RadialDistribution').index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave dataset out Cross validation - on LUAD and LINCS (2-folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "datasets = [\"LUAD\", \"LINCS\"]\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType = \"normalized\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"highRepOverlap\"\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "profileLevel = \"treatment\"\n",
    "#'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "\n",
    "(\n",
    "    mergProf_treatLevel_LI,\n",
    "    cp_features_LI,\n",
    "    l1k_features_LI,\n",
    ") = read_paired_treatment_level_profiles(\n",
    "    procProf_dir, \"LINCS\", profileType, filter_perts, 1\n",
    ")\n",
    "\n",
    "(\n",
    "    mergProf_treatLevel_LU,\n",
    "    cp_features_LU,\n",
    "    l1k_features_LU,\n",
    ") = read_paired_treatment_level_profiles(\n",
    "    procProf_dir, \"LUAD\", profileType, filter_perts, 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(cp_features_LU),\n",
    "    len(cp_features_LI),\n",
    "    len(set(cp_features_LU) & set(cp_features_LI)),\n",
    ")\n",
    "print(\n",
    "    len(l1k_features_LU),\n",
    "    len(l1k_features_LI),\n",
    "    len(set(l1k_features_LU) & set(l1k_features_LI)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_features_unionFs = list(set(cp_features_LU).union(set(cp_features_LI)))\n",
    "l1k_features_unionFs = list(set(l1k_features_LU).union(set(l1k_features_LI)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cp_features_unionFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cp_features_allOverlappingFs = list(\n",
    "    set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs)\n",
    ")\n",
    "l1k_features_allOverlappingFs = list(\n",
    "    set(l1k_features_LU) & set(l1k_features_LI) & set(l1k_features_unionFs)\n",
    ")\n",
    "\n",
    "len(cp_features_allOverlappingFs)\n",
    "# cp_features_LU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# cp_features=list(set(cp_features_LU) & set(cp_features_LI))\n",
    "# l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI))\n",
    "# profileType='normalized'\n",
    "\n",
    "cp_features=list(set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs))\n",
    "l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI)& set(l1k_features_unionFs))\n",
    "profileType='normalized_variable_selected_union'\n",
    "\n",
    "\n",
    "####################\n",
    "l1k_LU=mergProf_treatLevel_LU[[pertColName]+l1k_features]\n",
    "cp_LU=mergProf_treatLevel_LU[[pertColName]+cp_features]\n",
    "\n",
    "l1k_LI=mergProf_treatLevel_LI[[pertColName]+l1k_features]\n",
    "cp_LI=mergProf_treatLevel_LI[[pertColName]+cp_features]\n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "l1k_scaled_LU=l1k_LU.copy();l1k_scaled_LI=l1k_LI.copy()\n",
    "l1k_scaled_LU[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LU[l1k_features].values)\n",
    "l1k_scaled_LI[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LI[l1k_features].values)\n",
    "\n",
    "cp_scaled_LU=cp_LU.copy();cp_scaled_LI=cp_LI.copy()\n",
    "cp_scaled_LU[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LU[cp_features].values.astype('float64'))\n",
    "cp_scaled_LI[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LI[cp_features].values.astype('float64'))\n",
    "\n",
    "fold1=[[cp_scaled_LU,l1k_scaled_LU],[cp_scaled_LI,l1k_scaled_LI]]\n",
    "fold2=[[cp_scaled_LI,l1k_scaled_LI],[cp_scaled_LU,l1k_scaled_LU]]\n",
    "\n",
    "\n",
    "pred_df=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "pred_df_rand=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "\n",
    "# for dt in [fold1,fold2]:\n",
    "for n,dt in zip([0,1],[fold1,fold2]):\n",
    "    \n",
    "    for model in [\"Ridge\"]:#[\"Lasso\",\"MLP\",\"Ridge\"]:    \n",
    "\n",
    "        ##############################\n",
    "        ii=0\n",
    "        for l in l1k_features:\n",
    "            ii+=1\n",
    "            print(ii)\n",
    "\n",
    "            X_train, X_test = dt[0][0][cp_features].values, dt[1][0][cp_features].values\n",
    "            y_train, y_test = dt[0][1][l].values, dt[1][1][l]#.values\n",
    "\n",
    "            if model==\"Lasso\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas1 = np.linspace(0, 0.2, 20)\n",
    "                alphas2 = np.linspace(0.2, 0.5, 10)[1:]\n",
    "                alphas=np.concatenate((alphas1,alphas2))\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                lasso_cv = linear_model.LassoCV(alphas=alphas, random_state=0, max_iter=1000,selection='random')\n",
    "\n",
    "                lasso_cv.fit(X_train, y_train)  \n",
    "                r2_score=lasso_cv.score(X_test, y_test.values) \n",
    "                r2_rand=lasso_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "\n",
    "            if model==\"Ridge\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas = np.linspace(0.1, 0.2, 20)\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                ridge_cv = linear_model.RidgeCV(alphas=alphas)\n",
    "\n",
    "                ridge_cv.fit(X_train, y_train)  \n",
    "                r2_score=ridge_cv.score(X_test, y_test.values) \n",
    "                r2_rand=ridge_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "                \n",
    "            elif model==\"MLP\":\n",
    "#                 scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "    #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)  \n",
    "                mlp_gs = MLPRegressor(activation='logistic',max_iter=500)\n",
    "                parameter_space = {\n",
    "                    'hidden_layer_sizes': [(50,),(10,30,10),(50,10),(50,10,10)],\n",
    "                    'alpha': [0.0001, 0.05,0.01,0.2],\n",
    "                    'early_stopping':[True,False]\n",
    "                }\n",
    "\n",
    "                clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=2)\n",
    "                clf.fit(X_train, y_train)  \n",
    "                r2_score=clf.score(X_test, y_test.values)    \n",
    "                r2_rand=clf.score(X_test, y_test.sample(frac = 1).values)\n",
    "    \n",
    "            pred_df.loc[n,l]=r2_score\n",
    "            pred_df_rand.loc[n,l]=r2_rand\n",
    "\n",
    "    \n",
    "########################### mapping prob_ids to genes names    \n",
    "meta=pd.read_csv(\"../affy_probe_gene_mapping.txt\",delimiter=\"\\t\",header=None, names=[\"probe_id\", \"gene\"])\n",
    "meta_gene_probID=meta.set_index('probe_id')\n",
    "d = dict(zip(meta_gene_probID.index, meta_gene_probID['gene']))\n",
    "pred_df = pred_df.rename(columns=d)    \n",
    "pred_df_rand = pred_df_rand.rename(columns=d)  \n",
    "\n",
    "pred_df.loc[0,'DT']='LUAD-LINCS'\n",
    "pred_df.loc[1,'DT']='LINCS-LUAD'\n",
    "\n",
    "pred_df_rand.loc[0,'DT']='LUAD-LINCS'\n",
    "pred_df_rand.loc[1,'DT']='LINCS-LUAD'\n",
    "\n",
    "meltedPredDF=pd.melt(pred_df,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF_rand=pd.melt(pred_df_rand,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF['d']=\"n-folds\"\n",
    "meltedPredDF_rand['d']=\"random\"\n",
    "\n",
    "filename=results_dir+'/SingleGenePred/scores_cross_dts.xlsx'\n",
    "\n",
    "profTypeAbbrev=''.join([s[0] for s in profileType.split('_')])\n",
    "\n",
    "saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "                             model+'-'+profTypeAbbrev+'-'+f+'-ht')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = results_dir + \"/SingleGenePred/scores_cross_dts.xlsx\"\n",
    "\n",
    "profTypeAbbrev = \"\".join([s[0] for s in profileType.split(\"_\")])\n",
    "\n",
    "saveAsNewSheetToExistingFile(\n",
    "    filename,\n",
    "    pd.concat([meltedPredDF], ignore_index=True),\n",
    "    model + \"-\" + profTypeAbbrev + \"-\" + f + \"-ht\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave dataset out Cross validation - on CDRP-bio and LINCS (2-folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "datasets = [\"CDRP-bio\", \"LINCS\"]\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType = \"normalized\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"highRepOverlap\"\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "profileLevel = \"treatment\"\n",
    "#'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "(\n",
    "    mergProf_treatLevel_LI,\n",
    "    cp_features_LI,\n",
    "    l1k_features_LI,\n",
    ") = read_paired_treatment_level_profiles(\n",
    "    procProf_dir, \"LINCS\", profileType, filter_perts, 1\n",
    ")\n",
    "\n",
    "(\n",
    "    mergProf_treatLevel_LU,\n",
    "    cp_features_LU,\n",
    "    l1k_features_LU,\n",
    ") = read_paired_treatment_level_profiles(\n",
    "    procProf_dir, \"CDRP-bio\", profileType, filter_perts, 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_features_unionFs = list(set(cp_features_LU).union(set(cp_features_LI)))\n",
    "l1k_features_unionFs = list(set(l1k_features_LU).union(set(l1k_features_LI)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# cp_features=list(set(cp_features_LU) & set(cp_features_LI))\n",
    "# l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI))\n",
    "# profileType='normalized'\n",
    "\n",
    "cp_features=list(set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs))\n",
    "l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI)& set(l1k_features_unionFs))\n",
    "profileType='normalized_variable_selected_union'\n",
    "\n",
    "\n",
    "####################\n",
    "l1k_LU=mergProf_treatLevel_LU[[pertColName]+l1k_features]\n",
    "cp_LU=mergProf_treatLevel_LU[[pertColName]+cp_features]\n",
    "\n",
    "l1k_LI=mergProf_treatLevel_LI[[pertColName]+l1k_features]\n",
    "cp_LI=mergProf_treatLevel_LI[[pertColName]+cp_features]\n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "l1k_scaled_LU=l1k_LU.copy();l1k_scaled_LI=l1k_LI.copy()\n",
    "l1k_scaled_LU[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LU[l1k_features].values)\n",
    "l1k_scaled_LI[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LI[l1k_features].values)\n",
    "\n",
    "cp_scaled_LU=cp_LU.copy();cp_scaled_LI=cp_LI.copy()\n",
    "cp_scaled_LU[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LU[cp_features].values.astype('float64'))\n",
    "cp_scaled_LI[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LI[cp_features].values.astype('float64'))\n",
    "\n",
    "fold1=[[cp_scaled_LU,l1k_scaled_LU],[cp_scaled_LI,l1k_scaled_LI]]\n",
    "fold2=[[cp_scaled_LI,l1k_scaled_LI],[cp_scaled_LU,l1k_scaled_LU]]\n",
    "\n",
    "\n",
    "pred_df=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "pred_df_rand=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "\n",
    "# for dt in [fold1,fold2]:\n",
    "for n,dt in zip([0,1],[fold1,fold2]):\n",
    "    \n",
    "    for model in [\"Lasso\"]:#[\"Lasso\",\"MLP\",\"Ridge\"]:    \n",
    "\n",
    "        ##############################\n",
    "        ii=0\n",
    "        for l in l1k_features:\n",
    "            ii+=1\n",
    "            print(ii)\n",
    "\n",
    "            X_train, X_test = dt[0][0][cp_features].values, dt[1][0][cp_features].values\n",
    "            y_train, y_test = dt[0][1][l].values, dt[1][1][l]#.values\n",
    "\n",
    "            if model==\"Lasso\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas1 = np.linspace(0, 0.2, 20)\n",
    "                alphas2 = np.linspace(0.2, 0.5, 10)[1:]\n",
    "                alphas=np.concatenate((alphas1,alphas2))\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                lasso_cv = linear_model.LassoCV(alphas=alphas, random_state=0, max_iter=1000,selection='random')\n",
    "\n",
    "                lasso_cv.fit(X_train, y_train)  \n",
    "                r2_score=lasso_cv.score(X_test, y_test.values) \n",
    "                r2_rand=lasso_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "\n",
    "            if model==\"Ridge\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas = np.linspace(0.1, 0.2, 20)\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                ridge_cv = linear_model.RidgeCV(alphas=alphas)\n",
    "\n",
    "                ridge_cv.fit(X_train, y_train)  \n",
    "                r2_score=ridge_cv.score(X_test, y_test.values) \n",
    "                r2_rand=ridge_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "                \n",
    "            elif model==\"MLP\":\n",
    "#                 scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "    #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)  \n",
    "                mlp_gs = MLPRegressor(activation='logistic',max_iter=500)\n",
    "                parameter_space = {\n",
    "                    'hidden_layer_sizes': [(50,),(10,30,10),(50,10),(50,10,10)],\n",
    "                    'alpha': [0.0001, 0.05,0.01,0.2],\n",
    "                    'early_stopping':[True,False]\n",
    "                }\n",
    "\n",
    "                clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=2)\n",
    "                clf.fit(X_train, y_train)  \n",
    "                r2_score=clf.score(X_test, y_test.values)    \n",
    "                r2_rand=clf.score(X_test, y_test.sample(frac = 1).values)\n",
    "    \n",
    "            pred_df.loc[n,l]=r2_score\n",
    "            pred_df_rand.loc[n,l]=r2_rand\n",
    "\n",
    "    \n",
    "########################### mapping prob_ids to genes names    \n",
    "meta=pd.read_csv(\"../affy_probe_gene_mapping.txt\",delimiter=\"\\t\",header=None, names=[\"probe_id\", \"gene\"])\n",
    "meta_gene_probID=meta.set_index('probe_id')\n",
    "d = dict(zip(meta_gene_probID.index, meta_gene_probID['gene']))\n",
    "pred_df = pred_df.rename(columns=d)    \n",
    "pred_df_rand = pred_df_rand.rename(columns=d)  \n",
    "\n",
    "pred_df.loc[0,'DT']='CDRPbio-LINCS'\n",
    "pred_df.loc[1,'DT']='LINCS-CDRPbio'\n",
    "\n",
    "pred_df_rand.loc[0,'DT']='CDRPbio-LINCS'\n",
    "pred_df_rand.loc[1,'DT']='LINCS-CDRPbio'\n",
    "\n",
    "meltedPredDF=pd.melt(pred_df,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF_rand=pd.melt(pred_df_rand,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF['d']=\"n-folds\"\n",
    "meltedPredDF_rand['d']=\"random\"\n",
    "\n",
    "filename=results_dir+'/SingleGenePred/scores_cross_dts_CD_LI.xlsx'\n",
    "\n",
    "profTypeAbbrev=''.join([s[0] for s in profileType.split('_')])\n",
    "\n",
    "saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "                             model+'-'+profTypeAbbrev+'-'+f+'-ht')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "438.212px",
    "left": "1507.78px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "# import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import pairwise_distances,mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import gc\n",
    "sys.path.insert(0, '../utils/') \n",
    "from readProfiles import *\n",
    "from pred_models import *\n",
    "from saveAsNewSheetToExistingFile import saveAsNewSheetToExistingFile\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "# from utils import networksEvol, tsne, readProfiles\n",
    "# import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "\n",
    "\n",
    "- CDRP-BBBC047-Bray-CP-GE (Cell line: U2OS):\n",
    "    * There are 30,430 and 21,782 unique compounds for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~4 , GE: ~3. \n",
    "    * 20,358 compounds are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (153386 , 1783) ,  ge:  (68120 , 977)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp:  (30430, 1786)   ,  ge:  (21782, 981) \n",
    "    * Merged Profiles Shape:                              (20358, 2766)\n",
    "    * After 'highRepOverlap' filter: CP: 30618 to 7892, l1k: 21069 to 2870, overlap:  3\n",
    "    \n",
    "    \n",
    "- CDRPBIO-BBBC036-Bray-CP-GE (Cell line: U2OS):\n",
    "    * There are 30,430 and 21,782 unique compounds for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~4 , GE: ~3. \n",
    "    * 20,358 compounds are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (153386 , 1783) ,  ge:  (68120 , 977)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp:  (30430, 1786)   ,  ge:  (21782, 981) \n",
    "    * Merged Profiles Shape:                   \n",
    "    * After 'highRepOverlap' filter: CP: 2239 to 312, l1k: 1535 to 448, overlap:  131\n",
    "\n",
    "\n",
    "- LUAD-BBBC041-Caicedo-CP-GE (Cell line: A549) : \n",
    "    * There are 593 and 529 unique alleles for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~8, GE: ~8.\n",
    "    * 525 alleles are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (6144 , 1783) ,  ge:  (4232 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp: (594, 1786) , ge: (529, 979) \n",
    "    * Merged Profiles Shape:                             (526, 2764)    \n",
    "    * After 'highRepOverlap' filter: CP: from 593 to 364, l1k: from  529  to  275, CP and l1k high rep overlap: 197\n",
    "    \n",
    "    \n",
    "- TA-ORF-BBBC037-Rohban-CP-GE (Cell line: U2OS) :\n",
    "    * There are 323 and 327 number of unique compounds for CP and GE datasets respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~5 , GE: ~2.\n",
    "    * 188 alleles are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures):         cp: (1920 , 1783) ,  ge: (729 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures+metadata):cp: (324, 1784)   ,  ge: (328, 979)\n",
    "    * Merged Profiles Shape:                                     (150, 2762)\n",
    "    * After 'highRepOverlap' filter: CP: from 324 to 218, l1k: from 327 to 78, CP and l1k high rep overlap:  36 \n",
    "    \n",
    "    \n",
    "- LINCS-Pilot1-CP-GE (Cell line: A549) :\n",
    "    * There are 1570 unique compounds across 6 doses for CP dataset\n",
    "    * There are x unique compounds across 6 doses for GE dataset\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~5 , GE: ~3.\n",
    "    * 6984 \"compounds-dose\" are present in both datasets. \n",
    "    * Replicate Level Shapes (nSamples x nFeatures):         cp: (52223 , 1747) ,  ge: (27837 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures+metadata):cp: (9395, 1748)   ,  ge: (8370, 979)\n",
    "    * Merged Profiles Shape:                                     (6984, 2726)\n",
    "    * After 'highRepOverlap' filter: CP: 9394 to 4647, l1k: 8369  to  2338, overlap:  1140\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "procProf_dir='/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/'\n",
    "results_dir='../results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of single GE expression levels based on the full CP profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "# datasets=['LUAD','TAORF','LINCS','CDRP-bio'];\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "# datasets=['TAORF','LUAD','LINCS', 'CDRP-bio']\n",
    "datasets=['LINCS']\n",
    "# DT_kfold={'LUAD':10, 'TAORF':5, 'LINCS':25, 'CDRP-bio':6,'CDRP':40}\n",
    "DT_kfold={'LUAD':9, 'TAORF':5, 'LINCS':10, 'CDRP-bio':6,'CDRP':40}\n",
    "\n",
    "from IPython.display import clear_output\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType='normalized_variable_selected'\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts=''\n",
    "repCorrFilePath='../results/RepCor/RepCorrDF.xlsx'\n",
    "\n",
    "filter_repCorr_params=[filter_perts,repCorrFilePath]\n",
    "\n",
    "################################################\n",
    "pertColName='PERT'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# def f(dataset):\n",
    "for dataset in datasets:\n",
    "    \n",
    "    \n",
    "    if dataset=='TAORF':     \n",
    "        filter_perts=''\n",
    "    else:\n",
    "        filter_perts='highRepOverlap'         \n",
    "    \n",
    "    if filter_perts:\n",
    "        f='filt'\n",
    "    else:\n",
    "        f=''\n",
    "    \n",
    "    \n",
    "    mergProf_treatLevel,cp_features,l1k_features = \\\n",
    "    read_paired_treatment_level_profiles(procProf_dir,dataset,profileType,filter_repCorr_params,1)\n",
    "\n",
    "    l1k=mergProf_treatLevel[[pertColName]+l1k_features]\n",
    "    cp=mergProf_treatLevel[[pertColName]+cp_features]\n",
    "\n",
    "\n",
    "    if dataset=='LINCS':     \n",
    "        cp['Compounds']=cp['PERT'].str[0:13]\n",
    "        l1k['Compounds']=l1k['PERT'].str[0:13]\n",
    "    else:\n",
    "        cp['Compounds']=cp['PERT']\n",
    "        l1k['Compounds']=l1k['PERT']      \n",
    "\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels=le.fit_transform(l1k['Compounds'].values)\n",
    "\n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled=l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values)\n",
    "    cp_scaled=cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(cp[cp_features].values.astype('float64'))\n",
    "    \n",
    "    if 0:\n",
    "        from sklearn.decomposition import PCA\n",
    "        covar_matrix = PCA(n_components = cp_scaled.shape[0])\n",
    "        covar_matrix.fit(cp[cp_features])\n",
    "#         variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "\n",
    "#         var=np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3)*100)\n",
    "\n",
    "        pca_feats=covar_matrix.transform(cp[cp_features])[:,:40]\n",
    "    \n",
    "    \n",
    "    for model in [\"MLP-keras\",\"Lasso\"]:#[\"Lasso\",\"MLP\",\"RFR\"]:    \n",
    "\n",
    "        if 1:\n",
    "            cp=cp_scaled.copy()\n",
    "            l1k=l1k_scaled.copy()\n",
    "\n",
    "        ##############################\n",
    "         \n",
    "        k_fold=DT_kfold[dataset]\n",
    "            \n",
    "        pred_df=pd.DataFrame(index=range(k_fold),columns=l1k_features)\n",
    "        pred_df_rand=pd.DataFrame(index=range(k_fold),columns=l1k_features)\n",
    "        ii=0\n",
    "        for l in l1k_features:\n",
    "            start0 = time.time()\n",
    "            ii+=1\n",
    "            print(ii)\n",
    "            if model==\"Lasso\":\n",
    "                scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels,1)  \n",
    "#                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "            elif model==\"MLP\":\n",
    "#                 scores,scores_rand=MLP_cv_plus_model_selection(pca_feats,l1k[l],k_fold,group_labels,0)\n",
    "#                 scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features].values,l1k[l].sample(frac = 1),k_fold,group_labels,0)\n",
    "                scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features].values,l1k[l],k_fold,group_labels,0)\n",
    "#                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)  \n",
    "            elif model==\"MLP-keras\":\n",
    "                scores,_=MLP_cv_plus_model_selection_keras(cp[cp_features].values,l1k[l],k_fold,group_labels,0)\n",
    "                scores_rand,_=MLP_cv_plus_model_selection_keras(cp[cp_features].values,l1k[l].sample(frac = 1),k_fold,group_labels,0)\n",
    "\n",
    "            elif model==\"SVR\":\n",
    "                scores,scores_rand=SVR_cv_plus_model_selection(cp[cp_features].values,l1k[l],k_fold,group_labels,1)\n",
    "            elif model==\"Ridge\":\n",
    "                scores,scores_rand=ridge_cv_plus_model_selection(cp[cp_features].values,l1k[l],k_fold,group_labels,1)\n",
    "            elif model==\"RFR\":\n",
    "                scores,scores_rand=RFR_cv_plus_model_selection(cp[cp_features].values,l1k[l],k_fold,group_labels,1)  \n",
    "            \n",
    "            pred_df[l]=scores\n",
    "            pred_df_rand[l]=scores_rand\n",
    "            gc.collect()\n",
    "#             clear_output(wait=True)\n",
    "            \n",
    "            print('time elapsed:',(time.time() - start0)/60)\n",
    "            \n",
    "        ########################### mapping prob_ids to genes names    \n",
    "        pred_df,_=rename_affyprobe_to_genename(pred_df,l1k_features,'../idmap.xlsx')\n",
    "        pred_df_rand,_=rename_affyprobe_to_genename(pred_df_rand,l1k_features,'../idmap.xlsx')\n",
    "        \n",
    "\n",
    "\n",
    "        meltedPredDF=pd.melt(pred_df).rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "        meltedPredDF_rand=pd.melt(pred_df_rand).rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "        meltedPredDF['d']=\"n-folds\"\n",
    "        meltedPredDF_rand['d']=\"random\"\n",
    "#         filename=results_dir+'/SingleGenePred/scores_hyperParam.xlsx'\n",
    "        filename=results_dir+'/SingleGenePred/scores_randomSeed_fixed.xlsx'\n",
    "\n",
    "        \n",
    "        profTypeAbbrev=''.join([s[0] for s in profileType.split('_')])\n",
    "        \n",
    "        saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "                                     model+'-'+dataset+'-'+profTypeAbbrev+'-'+f+'-'+str(k_fold)+'-ht')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prediction of single CP features based on the full GE profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "# datasets=['LINCS', 'CDRP-bio'];\n",
    "datasets=['CDRP-bio'];\n",
    "DT_kfold={'LUAD':9, 'TAORF':5, 'LINCS':25, 'CDRP-bio':6,'CDRP':40}\n",
    "\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "# 'normalized_feature_select_dmso'\n",
    "profileType='normalized'\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "# filter_perts='highRepOverlap'\n",
    "filter_perts=''\n",
    "repCorrFilePath='../results/RepCor/RepCorrDF.xlsx'\n",
    "\n",
    "filter_repCorr_params=[filter_perts,repCorrFilePath]\n",
    "\n",
    "################################################\n",
    "pertColName='PERT'\n",
    "\n",
    "if filter_perts:\n",
    "    f='filt'\n",
    "else:\n",
    "    f=''\n",
    "    \n",
    "# def f(dataset):\n",
    "for dataset in datasets:\n",
    "            \n",
    "    mergProf_treatLevel,cp_features,l1k_features = \\\n",
    "    read_paired_treatment_level_profiles(procProf_dir,dataset,profileType,filter_repCorr_params,1)\n",
    "\n",
    "    l1k=mergProf_treatLevel[[pertColName]+l1k_features]\n",
    "    cp=mergProf_treatLevel[[pertColName]+cp_features]\n",
    "\n",
    "        \n",
    "    if dataset=='LINCS':     \n",
    "        cp['Compounds']=cp['PERT'].str[0:13]\n",
    "        l1k['Compounds']=l1k['PERT'].str[0:13]\n",
    "    else:\n",
    "        cp['Compounds']=cp['PERT']\n",
    "        l1k['Compounds']=l1k['PERT']      \n",
    "\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels=le.fit_transform(l1k['Compounds'].values)        \n",
    "        \n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled=l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values)\n",
    "    cp_scaled=cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(cp[cp_features].values.astype('float64'))\n",
    "\n",
    "    for model in [\"MLP\"]:    \n",
    "    \n",
    "#         if model==\"MLP\":\n",
    "#             cp_scaled[cp_features] =preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit_transform(cp_scaled[cp_features].values)   \n",
    "#             cp_scaled[l1k_features] =preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit_transform(cp_scaled[l1k_features].values)           \n",
    "\n",
    "        if 1:\n",
    "            cp=cp_scaled.copy()\n",
    "            l1k=l1k_scaled.copy()\n",
    "\n",
    "        ##############################\n",
    "            \n",
    "            \n",
    "        k_fold=DT_kfold[dataset]\n",
    "#         k_fold=int(np.unique(group_labels).shape[0]/20)            \n",
    "        pred_df=pd.DataFrame(index=range(k_fold),columns=cp_features)\n",
    "        pred_df_rand=pd.DataFrame(index=range(k_fold),columns=cp_features)\n",
    "        ii=0\n",
    "        for c in cp_features:\n",
    "            start0 = time.time()\n",
    "            ii+=1\n",
    "            print(ii)            \n",
    "            if model==\"Lasso\":\n",
    "                scores,scores_rand=lasso_cv_plus_model_selection(l1k[l1k_features],cp[c],k_fold,group_labels,1)\n",
    "            elif model==\"MLP\":\n",
    "                scores,scores_rand=MLP_cv_plus_model_selection_taorf_test(l1k[l1k_features].sample(frac = 1),cp[c],k_fold,group_labels,1)\n",
    "#                 scores,scores_rand=MLP_cv_plus_model_selection(l1k[l1k_features],cp[c],k_fold,group_labels,1)\n",
    "                \n",
    "#             sgsg\n",
    "            pred_df[c]=scores\n",
    "            pred_df_rand[c]=scores_rand\n",
    "\n",
    "            gc.collect()\n",
    "            print('time elapsed:',(time.time() - start0)/60)            \n",
    "            \n",
    "        meltedPredDF=pd.melt(pred_df).rename(columns={'variable':'CP-Features','value':'pred score'})\n",
    "        meltedPredDF_rand=pd.melt(pred_df_rand).rename(columns={'variable':'CP-Features','value':'pred score'})\n",
    "        meltedPredDF['d']=\"n-folds\"\n",
    "        meltedPredDF_rand['d']=\"random\"\n",
    "#         filename='../../results/SingleCPfeatPred/scores.xlsx'\n",
    "        filename=results_dir+'/SingleCPfeatPred/scores_randomSeed_fixed.xlsx'\n",
    "        \n",
    "        profTypeAbbrev=''.join([s[0] for s in profileType.split('_')])        \n",
    "        \n",
    "        saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "                                     model+'-'+dataset+'-'+profTypeAbbrev+'-'+f+'-'+str(k_fold)+'-ht_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### CP Category specific scores for single gene prediction - based on Lasso  regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "from warnings import simplefilter\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "import gc\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "# datasets=['LUAD', 'TAORF', 'LINCS', 'CDRP-bio'];\n",
    "datasets=['LUAD'];\n",
    "\n",
    "# DT_kfold={'LUAD':10, 'TAORF':5, 'LINCS':20, 'CDRP-bio':20,'CDRP':40}\n",
    "\n",
    "DT_kfold={'LUAD':9, 'TAORF':5, 'LINCS':25, 'CDRP-bio':6,'CDRP':40}\n",
    "\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "# 'normalized_feature_select_dmso'\n",
    "profileType='normalized'\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts='highRepOverlap'\n",
    "repCorrFilePath='../results/RepCor/RepCorrDF.xlsx'\n",
    "\n",
    "filter_repCorr_params=[filter_perts,repCorrFilePath]\n",
    "\n",
    "################################################\n",
    "pertColName='PERT'\n",
    "profileLevel='treatment'; #'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f='filt'\n",
    "else:\n",
    "    f=''\n",
    "    \n",
    "regr = MLPRegressor(hidden_layer_sizes = (50,10),activation='logistic',\\\n",
    "alpha=0.7,early_stopping=True,n_iter_no_change=50,max_iter=1000)\n",
    " \n",
    "    \n",
    "# def f(dataset):\n",
    "for dataset in datasets:\n",
    "        \n",
    "    mergProf_treatLevel,cp_features,l1k_features = \\\n",
    "    read_paired_treatment_level_profiles(procProf_dir,dataset,profileType,filter_repCorr_params,1)\n",
    "\n",
    "\n",
    "    l1k=mergProf_treatLevel[[pertColName]+l1k_features]\n",
    "    cp=mergProf_treatLevel[[pertColName]+cp_features]\n",
    "\n",
    "        \n",
    "    if dataset=='LINCS':     \n",
    "        cp['Compounds']=cp['PERT'].str[0:13]\n",
    "        l1k['Compounds']=l1k['PERT'].str[0:13]\n",
    "    else:\n",
    "        cp['Compounds']=cp['PERT']\n",
    "        l1k['Compounds']=l1k['PERT']      \n",
    "\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels=le.fit_transform(l1k['Compounds'].values)        \n",
    "        \n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled=l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values.astype('float64'))\n",
    "    cp_scaled=cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(cp[cp_features].values.astype('float64'))\n",
    "\n",
    "    if 1:\n",
    "        cp=cp_scaled.copy()\n",
    "        l1k=l1k_scaled.copy()\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    k_fold=DT_kfold[dataset]\n",
    "    split_obj=GroupKFold(n_splits=k_fold)     \n",
    "    \n",
    "    Channelss=['DNA','RNA','AGP','Mito','ER']\n",
    "    featureGroups=['Texture','Intensity','RadialDistribution']\n",
    "    relationMat_mpCat=pd.DataFrame(index=l1k_features)\n",
    "    for ch in range(len(Channelss)):\n",
    "        for f in range(len(featureGroups)):\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(ch,f)\n",
    "            \n",
    "            start0 = time.time()\n",
    "            selectedCols=cp.columns[cp.columns.str.contains(Channelss[ch]) &\\\n",
    "                           cp.columns.str.contains(featureGroups[f]) &\\\n",
    "                           cp.columns.str.contains(\"Cells_|Cytoplasm_|Nuclei_\")].tolist();\n",
    "            if selectedCols!=[]:\n",
    "                ii=0\n",
    "                for l in l1k_features:\n",
    "                    ii+=1\n",
    "                    print(ii)\n",
    "#                     start0 = time.time()\n",
    "                    scores,_=MLP_cv_plus_model_selection_keras(cp[selectedCols].values,l1k[l],k_fold,group_labels,0)\n",
    "#                     scores,scores_rand=lasso_cv_plus_model_selection(cp[selectedCols],l1k[l],k_fold,group_labels,0)\n",
    "#                     # Perform k-fold cross validation\n",
    "#                     scores = cross_val_score(regr, cp[selectedCols].values, l1k[l].values, groups=group_labels,cv=split_obj,n_jobs=k_fold)\n",
    "                    relationMat_mpCat.loc[l,Channelss[ch]+'_'+featureGroups[f]]=np.median(scores)\n",
    "                    gc.collect()\n",
    "                print('time elapsed:',(time.time() - start0)/60)\n",
    "            \n",
    "    Channelss=['Nuclei','Cytoplasm','Cells']\n",
    "    featureGroups=['AreaShape']\n",
    "    \n",
    "    for ch in range(len(Channelss)):\n",
    "        for f in range(len(featureGroups)): \n",
    "            clear_output(wait=True)\n",
    "            print(ch,f)\n",
    "            selectedCols=cp.columns[cp.columns.str.contains(Channelss[ch]) &\\\n",
    "                           cp.columns.str.contains(featureGroups[f]) &\\\n",
    "                           cp.columns.str.contains(\"Cells_|Cytoplasm_|Nuclei_\")].tolist();\n",
    "            if selectedCols!=[]:\n",
    "                for l in l1k_features:\n",
    "                    scores,_=MLP_cv_plus_model_selection_keras(cp[selectedCols].values,l1k[l],k_fold,group_labels,0)                    \n",
    "#                     scores,scores_rand=lasso_cv_plus_model_selection(cp[selectedCols],l1k[l],k_fold,group_labels,0)\n",
    "#                     scores,scores_rand=MLP_cv_plus_model_selection(cp[selectedCols],l1k[l],k_fold,group_labels,0)\n",
    "#                     scores = cross_val_score(regr, cp[selectedCols].values, l1k[l].values, groups=group_labels,cv=split_obj,n_jobs=k_fold)                    \n",
    "                    relationMat_mpCat.loc[l,Channelss[ch]+'_'+featureGroups[f]]=np.median(scores)\n",
    "                    gc.collect()\n",
    "    ########################### mapping prob_ids to genes names    \n",
    "#     meta=pd.read_csv(\"/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/metadata/affy_probe_gene_mapping.txt\",delimiter=\"\\t\",header=None, names=[\"probe_id\", \"gene\"])\n",
    "#     meta_gene_probID=meta.set_index('probe_id')\n",
    "#     d = dict(zip(meta_gene_probID.index, meta_gene_probID['gene']))\n",
    "#     relationMat_mpCat = relationMat_mpCat.rename(index=d)    \n",
    "\n",
    "    filename=results_dir+'/SingleGenePred_cpCategoryMap/cat_scores_maps.xlsx'\n",
    "\n",
    "    profTypeAbbrev=''.join([s[0] for s in profileType.split('_')])        \n",
    "\n",
    "    saveAsNewSheetToExistingFile(filename,relationMat_mpCat,dataset+'-'+str(k_fold)+'-MLP-ht-corrected')    \n",
    "#     saveAsNewSheetToExistingFile(filename,relationMat_mpCat,dataset+'-'+str(k_fold)+'-MLP-keras-ht')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Leave dataset out Cross validation - on LUAD and LINCS (2-folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "datasets=['LUAD','LINCS'];\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType='normalized'\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts='highRepOverlap'\n",
    "repCorrFilePath='../results/RepCor/RepCorrDF.xlsx'\n",
    "\n",
    "filter_repCorr_params=[filter_perts,repCorrFilePath]\n",
    "################################################\n",
    "pertColName='PERT'\n",
    "profileLevel='treatment'; #'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f='filt'\n",
    "else:\n",
    "    f=''\n",
    "    \n",
    "\n",
    "mergProf_treatLevel_LI,cp_features_LI,l1k_features_LI= \\\n",
    "read_paired_treatment_level_profiles(procProf_dir,'LINCS',profileType,filter_repCorr_params,1)\n",
    "\n",
    "mergProf_treatLevel_LU,cp_features_LU,l1k_features_LU = \\\n",
    "read_paired_treatment_level_profiles(procProf_dir,'LUAD',profileType,filter_repCorr_params,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(len(cp_features_LU),len(cp_features_LI),len(set(cp_features_LU) & set(cp_features_LI)))\n",
    "print(len(l1k_features_LU),len(l1k_features_LI),len(set(l1k_features_LU) & set(l1k_features_LI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cp_features_unionFs=list(set(cp_features_LU).union(set(cp_features_LI)))\n",
    "l1k_features_unionFs=list(set(l1k_features_LU).union(set(l1k_features_LI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(cp_features_unionFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cp_features_allOverlappingFs=list(set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs))\n",
    "l1k_features_allOverlappingFs=list(set(l1k_features_LU) & set(l1k_features_LI)& set(l1k_features_unionFs))\n",
    "\n",
    "len(cp_features_allOverlappingFs)\n",
    "# cp_features_LU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# cp_features=list(set(cp_features_LU) & set(cp_features_LI))\n",
    "# l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI))\n",
    "# profileType='normalized'\n",
    "\n",
    "cp_features=list(set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs))\n",
    "l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI)& set(l1k_features_unionFs))\n",
    "profileType='normalized_variable_selected_union'\n",
    "\n",
    "\n",
    "####################\n",
    "l1k_LU=mergProf_treatLevel_LU[[pertColName]+l1k_features]\n",
    "cp_LU=mergProf_treatLevel_LU[[pertColName]+cp_features]\n",
    "\n",
    "l1k_LI=mergProf_treatLevel_LI[[pertColName]+l1k_features]\n",
    "cp_LI=mergProf_treatLevel_LI[[pertColName]+cp_features]\n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "l1k_scaled_LU=l1k_LU.copy();l1k_scaled_LI=l1k_LI.copy()\n",
    "l1k_scaled_LU[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LU[l1k_features].values)\n",
    "l1k_scaled_LI[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LI[l1k_features].values)\n",
    "\n",
    "cp_scaled_LU=cp_LU.copy();cp_scaled_LI=cp_LI.copy()\n",
    "cp_scaled_LU[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LU[cp_features].values.astype('float64'))\n",
    "cp_scaled_LI[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LI[cp_features].values.astype('float64'))\n",
    "\n",
    "fold1=[[cp_scaled_LU,l1k_scaled_LU],[cp_scaled_LI,l1k_scaled_LI]]\n",
    "fold2=[[cp_scaled_LI,l1k_scaled_LI],[cp_scaled_LU,l1k_scaled_LU]]\n",
    "\n",
    "\n",
    "pred_df=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "pred_df_rand=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "\n",
    "# for dt in [fold1,fold2]:\n",
    "for n,dt in zip([0,1],[fold1,fold2]):\n",
    "    \n",
    "    for model in [\"Lasso\"]:#[\"Lasso\",\"MLP\",\"Ridge\"]:    \n",
    "\n",
    "        ##############################\n",
    "        ii=0\n",
    "        for l in l1k_features:\n",
    "            ii+=1\n",
    "            print(ii)\n",
    "\n",
    "            X_train, X_test = dt[0][0][cp_features].values, dt[1][0][cp_features].values\n",
    "            y_train, y_test = dt[0][1][l].values, dt[1][1][l]#.values\n",
    "\n",
    "            if model==\"Lasso\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas1 = np.linspace(0, 0.2, 20)\n",
    "                alphas2 = np.linspace(0.2, 5, 100)[1:]\n",
    "                alphas=np.concatenate((alphas1,alphas2))\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                lasso_cv = linear_model.LassoCV(alphas=alphas, random_state=0, max_iter=1000,selection='random')\n",
    "\n",
    "                lasso_cv.fit(X_train, y_train)  \n",
    "                r2_score=lasso_cv.score(X_test, y_test.values) \n",
    "                r2_rand=lasso_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "\n",
    "            if model==\"Ridge\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas = np.linspace(0.1, 0.2, 20)\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                ridge_cv = linear_model.RidgeCV(alphas=alphas)\n",
    "\n",
    "                ridge_cv.fit(X_train, y_train)  \n",
    "                r2_score=ridge_cv.score(X_test, y_test.values) \n",
    "                r2_rand=ridge_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "                \n",
    "            elif model==\"MLP\":\n",
    "#                 scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "    #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)  \n",
    "                mlp_gs = MLPRegressor(activation='logistic',max_iter=500)\n",
    "                parameter_space = {\n",
    "                    'hidden_layer_sizes': [(50,),(10,30,10),(50,10),(50,10,10)],\n",
    "                    'alpha': [0.0001, 0.05,0.01,0.2],\n",
    "#                     'early_stopping':[True,False]\n",
    "                }\n",
    "\n",
    "                clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=2)\n",
    "                clf.fit(X_train, y_train)  \n",
    "                r2_score=clf.score(X_test, y_test.values)    \n",
    "                r2_rand=clf.score(X_test, y_test.sample(frac = 1).values)\n",
    "    \n",
    "            pred_df.loc[n,l]=r2_score\n",
    "            pred_df_rand.loc[n,l]=r2_rand\n",
    "\n",
    "    \n",
    "########################### mapping prob_ids to genes names    \n",
    "# meta=pd.read_csv(\"/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/metadata/affy_probe_gene_mapping.txt\",delimiter=\"\\t\",header=None, names=[\"probe_id\", \"gene\"])\n",
    "# meta_gene_probID=meta.set_index('probe_id')\n",
    "# d = dict(zip(meta_gene_probID.index, meta_gene_probID['gene']))\n",
    "# pred_df = pred_df.rename(columns=d)    \n",
    "# pred_df_rand = pred_df_rand.rename(columns=d)  \n",
    "\n",
    "pred_df,_=rename_affyprobe_to_genename(pred_df,l1k_features,'../idmap.xlsx')\n",
    "pred_df_rand,_=rename_affyprobe_to_genename(pred_df_rand,l1k_features,'../idmap.xlsx')\n",
    "\n",
    "pred_df.loc[0,'DT']='LUAD-LINCS'\n",
    "pred_df.loc[1,'DT']='LINCS-LUAD'\n",
    "\n",
    "pred_df_rand.loc[0,'DT']='LUAD-LINCS'\n",
    "pred_df_rand.loc[1,'DT']='LINCS-LUAD'\n",
    "\n",
    "meltedPredDF=pd.melt(pred_df,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF_rand=pd.melt(pred_df_rand,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF['d']=\"n-folds\"\n",
    "meltedPredDF_rand['d']=\"random\"\n",
    "\n",
    "filename=results_dir+'/SingleGenePred/scores_cross_dts.xlsx'\n",
    "\n",
    "profTypeAbbrev=''.join([s[0] for s in profileType.split('_')])\n",
    "\n",
    "saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "                             model+'-'+profTypeAbbrev+'-'+f+'-ht')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Leave dataset out Cross validation - on CDRP-bio and LINCS (2-folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "datasets=['CDRP-bio','LINCS'];\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType='normalized'\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts='highRepOverlap'\n",
    "\n",
    "################################################\n",
    "pertColName='PERT'\n",
    "profileLevel='treatment'; #'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f='filt'\n",
    "else:\n",
    "    f=''\n",
    "    \n",
    "mergProf_treatLevel_LI,cp_features_LI,l1k_features_LI= \\\n",
    "read_paired_treatment_level_profiles(procProf_dir,'LINCS',profileType,filter_perts,1)\n",
    "\n",
    "mergProf_treatLevel_LU,cp_features_LU,l1k_features_LU=\\\n",
    "read_paired_treatment_level_profiles(procProf_dir,'CDRP-bio',profileType,filter_perts,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cp_features_unionFs=list(set(cp_features_LU).union(set(cp_features_LI)))\n",
    "l1k_features_unionFs=list(set(l1k_features_LU).union(set(l1k_features_LI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# cp_features=list(set(cp_features_LU) & set(cp_features_LI))\n",
    "# l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI))\n",
    "# profileType='normalized'\n",
    "\n",
    "cp_features=list(set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs))\n",
    "l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI)& set(l1k_features_unionFs))\n",
    "profileType='normalized_variable_selected_union'\n",
    "\n",
    "\n",
    "####################\n",
    "l1k_LU=mergProf_treatLevel_LU[[pertColName]+l1k_features]\n",
    "cp_LU=mergProf_treatLevel_LU[[pertColName]+cp_features]\n",
    "\n",
    "l1k_LI=mergProf_treatLevel_LI[[pertColName]+l1k_features]\n",
    "cp_LI=mergProf_treatLevel_LI[[pertColName]+cp_features]\n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "l1k_scaled_LU=l1k_LU.copy();l1k_scaled_LI=l1k_LI.copy()\n",
    "l1k_scaled_LU[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LU[l1k_features].values)\n",
    "l1k_scaled_LI[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LI[l1k_features].values)\n",
    "\n",
    "cp_scaled_LU=cp_LU.copy();cp_scaled_LI=cp_LI.copy()\n",
    "cp_scaled_LU[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LU[cp_features].values.astype('float64'))\n",
    "cp_scaled_LI[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LI[cp_features].values.astype('float64'))\n",
    "\n",
    "fold1=[[cp_scaled_LU,l1k_scaled_LU],[cp_scaled_LI,l1k_scaled_LI]]\n",
    "fold2=[[cp_scaled_LI,l1k_scaled_LI],[cp_scaled_LU,l1k_scaled_LU]]\n",
    "\n",
    "\n",
    "pred_df=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "pred_df_rand=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "\n",
    "# for dt in [fold1,fold2]:\n",
    "for n,dt in zip([0,1],[fold1,fold2]):\n",
    "    \n",
    "    for model in [\"Lasso\"]:#[\"Lasso\",\"MLP\",\"Ridge\"]:    \n",
    "\n",
    "        ##############################\n",
    "        ii=0\n",
    "        for l in l1k_features:\n",
    "            ii+=1\n",
    "            print(ii)\n",
    "\n",
    "            X_train, X_test = dt[0][0][cp_features].values, dt[1][0][cp_features].values\n",
    "            y_train, y_test = dt[0][1][l].values, dt[1][1][l]#.values\n",
    "\n",
    "            if model==\"Lasso\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas1 = np.linspace(0, 0.2, 20)\n",
    "                alphas2 = np.linspace(0.2, 0.5, 10)[1:]\n",
    "                alphas=np.concatenate((alphas1,alphas2))\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                lasso_cv = linear_model.LassoCV(alphas=alphas, random_state=0, max_iter=1000,selection='random')\n",
    "\n",
    "                lasso_cv.fit(X_train, y_train)  \n",
    "                r2_score=lasso_cv.score(X_test, y_test.values) \n",
    "                r2_rand=lasso_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "\n",
    "            if model==\"Ridge\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas = np.linspace(0.1, 0.2, 20)\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                ridge_cv = linear_model.RidgeCV(alphas=alphas)\n",
    "\n",
    "                ridge_cv.fit(X_train, y_train)  \n",
    "                r2_score=ridge_cv.score(X_test, y_test.values) \n",
    "                r2_rand=ridge_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "                \n",
    "            elif model==\"MLP\":\n",
    "#                 scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "    #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)  \n",
    "                mlp_gs = MLPRegressor(activation='logistic',max_iter=500)\n",
    "                parameter_space = {\n",
    "                    'hidden_layer_sizes': [(50,),(10,30,10),(50,10),(50,10,10)],\n",
    "                    'alpha': [0.0001, 0.05,0.01,0.2],\n",
    "                    'early_stopping':[True,False]\n",
    "                }\n",
    "\n",
    "                clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=2)\n",
    "                clf.fit(X_train, y_train)  \n",
    "                r2_score=clf.score(X_test, y_test.values)    \n",
    "                r2_rand=clf.score(X_test, y_test.sample(frac = 1).values)\n",
    "    \n",
    "            pred_df.loc[n,l]=r2_score\n",
    "            pred_df_rand.loc[n,l]=r2_rand\n",
    "\n",
    "    \n",
    "########################### mapping prob_ids to genes names    \n",
    "meta=pd.read_csv(\"/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/metadata/affy_probe_gene_mapping.txt\",delimiter=\"\\t\",header=None, names=[\"probe_id\", \"gene\"])\n",
    "meta_gene_probID=meta.set_index('probe_id')\n",
    "d = dict(zip(meta_gene_probID.index, meta_gene_probID['gene']))\n",
    "pred_df = pred_df.rename(columns=d)    \n",
    "pred_df_rand = pred_df_rand.rename(columns=d)  \n",
    "\n",
    "pred_df.loc[0,'DT']='CDRPbio-LINCS'\n",
    "pred_df.loc[1,'DT']='LINCS-CDRPbio'\n",
    "\n",
    "pred_df_rand.loc[0,'DT']='CDRPbio-LINCS'\n",
    "pred_df_rand.loc[1,'DT']='LINCS-CDRPbio'\n",
    "\n",
    "meltedPredDF=pd.melt(pred_df,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF_rand=pd.melt(pred_df_rand,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF['d']=\"n-folds\"\n",
    "meltedPredDF_rand['d']=\"random\"\n",
    "\n",
    "filename=results_dir+'/SingleGenePred/scores_cross_dts_CD_LI.xlsx'\n",
    "\n",
    "profTypeAbbrev=''.join([s[0] for s in profileType.split('_')])\n",
    "\n",
    "saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "                             model+'-'+profTypeAbbrev+'-'+f+'-ht')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model+'-'+profTypeAbbrev+'-'+f+'-ht'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mergProf_treatLevel_LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "438.212px",
    "left": "1507.78px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

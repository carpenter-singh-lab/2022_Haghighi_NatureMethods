{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "# import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import pairwise_distances,mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import gc\n",
    "sys.path.insert(0, '../utils/') \n",
    "from readProfiles import *\n",
    "from pred_models import *\n",
    "from saveAsNewSheetToExistingFile import saveAsNewSheetToExistingFile\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=100,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY310,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "\n",
    "\n",
    "- CDRP-BBBC047-Bray-CP-GE (Cell line: U2OS):\n",
    "    * There are 30,430 and 21,782 unique compounds for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~4 , GE: ~3. \n",
    "    * 20,358 compounds are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (153386 , 1783) ,  ge:  (68120 , 977)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp:  (30430, 1786)   ,  ge:  (21782, 981) \n",
    "    * Merged Profiles Shape:                              (20358, 2766)\n",
    "    * After 'highRepOverlap' filter: CP: 30618 to 7892, l1k: 21069 to 2870, overlap:  3\n",
    "    \n",
    "    \n",
    "- CDRPBIO-BBBC036-Bray-CP-GE (Cell line: U2OS):\n",
    "    * There are 30,430 and 21,782 unique compounds for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~4 , GE: ~3. \n",
    "    * 20,358 compounds are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (153386 , 1783) ,  ge:  (68120 , 977)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp:  (30430, 1786)   ,  ge:  (21782, 981) \n",
    "    * Merged Profiles Shape:                   \n",
    "    * After 'highRepOverlap' filter: CP: 2239 to 312, l1k: 1535 to 448, overlap:  131\n",
    "\n",
    "\n",
    "- LUAD-BBBC041-Caicedo-CP-GE (Cell line: A549) : \n",
    "    * There are 593 and 529 unique alleles for CP and GE datasets, respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~8, GE: ~8.\n",
    "    * 525 alleles are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures): cp:  (6144 , 1783) ,  ge:  (4232 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures): cp: (594, 1786) , ge: (529, 979) \n",
    "    * Merged Profiles Shape:                             (526, 2764)    \n",
    "    * After 'highRepOverlap' filter: CP: from 593 to 364, l1k: from  529  to  275, CP and l1k high rep overlap: 197\n",
    "    \n",
    "    \n",
    "- TA-ORF-BBBC037-Rohban-CP-GE (Cell line: U2OS) :\n",
    "    * There are 323 and 327 number of unique compounds for CP and GE datasets respectively.\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~5 , GE: ~2.\n",
    "    * 188 alleles are present in both datasets.\n",
    "    * Replicate Level Shapes (nSamples x nFeatures):         cp: (1920 , 1783) ,  ge: (729 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures+metadata):cp: (324, 1784)   ,  ge: (328, 979)\n",
    "    * Merged Profiles Shape:                                     (150, 2762)\n",
    "    * After 'highRepOverlap' filter: CP: from 324 to 218, l1k: from 327 to 78, CP and l1k high rep overlap:  36 \n",
    "    \n",
    "    \n",
    "- LINCS-Pilot1-CP-GE (Cell line: A549) :\n",
    "    * There are 1570 unique compounds across 6 doses for CP dataset\n",
    "    * There are x unique compounds across 6 doses for GE dataset\n",
    "    * Median number of replicates for each dataset is as follows: CP: ~5 , GE: ~3.\n",
    "    * 6984 \"compounds-dose\" are present in both datasets. \n",
    "    * Replicate Level Shapes (nSamples x nFeatures):         cp: (52223 , 1747) ,  ge: (27837 , 978)\n",
    "    * Treatment Level Shapes (nSamples x nFeatures+metadata):cp: (9395, 1748)   ,  ge: (8370, 979)\n",
    "    * Merged Profiles Shape:                                     (6984, 2726)\n",
    "    * After 'highRepOverlap' filter: CP: 9394 to 4647, l1k: 8369  to  2338, overlap:  1140\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procProf_dir='/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/'\n",
    "procProf_dir = \"/home/ubuntu/gallery/cpg0003-rosetta/broad/workspace/\"\n",
    "results_dir = \"../results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prediction of single GE expression levels based on the full CP profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "# datasets=['LUAD','TAORF','LINCS','CDRP-bio'];\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "# datasets=['TAORF','LUAD','LINCS', 'CDRP-bio']\n",
    "datasets = [\"LUAD\"]\n",
    "# DT_kfold={'LUAD':10, 'TAORF':5, 'LINCS':25, 'CDRP-bio':6,'CDRP':40}\n",
    "DT_kfold = {\"LUAD\": 9, \"TAORF\": 5, \"LINCS\": 10, \"CDRP-bio\": 6, \"CDRP\": 40}\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType = \"normalized_variable_selected\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"\"\n",
    "repCorrFilePath = \"../results/RepCor/RepCorrDF.xlsx\"\n",
    "\n",
    "filter_repCorr_params = [filter_perts, repCorrFilePath]\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    if dataset == \"TAORF\":\n",
    "        filter_perts = \"\"\n",
    "    else:\n",
    "        filter_perts = \"highRepOverlap\"\n",
    "\n",
    "    if filter_perts:\n",
    "        f = \"filt\"\n",
    "    else:\n",
    "        f = \"\"\n",
    "\n",
    "    mergProf_treatLevel, cp_features, l1k_features = read_paired_treatment_level_profiles(\n",
    "        procProf_dir, dataset, profileType, filter_repCorr_params, 1\n",
    "    )\n",
    "\n",
    "    l1k = mergProf_treatLevel[[pertColName] + l1k_features]\n",
    "    cp = mergProf_treatLevel[[pertColName] + cp_features]\n",
    "\n",
    "    if dataset == \"LINCS\":\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"].str[0:13]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"].str[0:13]\n",
    "    else:\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels = le.fit_transform(l1k[\"Compounds\"].values)\n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled = l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values)\n",
    "    cp_scaled = cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(cp[cp_features].values.astype(\"float64\"))\n",
    "\n",
    "    if 0:\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        covar_matrix = PCA(n_components=cp_scaled.shape[0])\n",
    "        covar_matrix.fit(cp[cp_features])\n",
    "        #         variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "        #         var=np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3)*100)\n",
    "        pca_feats = covar_matrix.transform(cp[cp_features])[:, :40]\n",
    "\n",
    "    for model in [\"MLP-keras\", \"Lasso\"]:  # [\"Lasso\",\"MLP\",\"RFR\"]:\n",
    "        if 1:\n",
    "            cp = cp_scaled.copy()\n",
    "            l1k = l1k_scaled.copy()\n",
    "\n",
    "        ##############################\n",
    "\n",
    "        k_fold = DT_kfold[dataset]\n",
    "\n",
    "        pred_df = pd.DataFrame(index=range(k_fold), columns=l1k_features)\n",
    "        pred_df_rand = pd.DataFrame(index=range(k_fold), columns=l1k_features)\n",
    "        ii = 0\n",
    "        for l in l1k_features:\n",
    "            start0 = time.time()\n",
    "            ii += 1\n",
    "            print(ii)\n",
    "            if model == \"Lasso\":\n",
    "                scores, scores_rand = lasso_cv_plus_model_selection(\n",
    "                    cp[cp_features], l1k[l], k_fold, group_labels, 1\n",
    "                )\n",
    "            #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "            elif model == \"MLP\":\n",
    "                #                 scores,scores_rand=MLP_cv_plus_model_selection(pca_feats,l1k[l],k_fold,group_labels,0)\n",
    "                #                 scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features].values,l1k[l].sample(frac = 1),k_fold,group_labels,0)\n",
    "                scores, scores_rand = MLP_cv_plus_model_selection(\n",
    "                    cp[cp_features].values, l1k[l], k_fold, group_labels, 0\n",
    "                )\n",
    "            #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "            elif model == \"MLP-keras\":\n",
    "                scores, _ = MLP_cv_plus_model_selection_keras(\n",
    "                    cp[cp_features].values, l1k[l], k_fold, group_labels, 0\n",
    "                )\n",
    "                scores_rand, _ = MLP_cv_plus_model_selection_keras(\n",
    "                    cp[cp_features].values, l1k[l].sample(frac=1), k_fold, group_labels, 0\n",
    "                )\n",
    "\n",
    "            elif model == \"SVR\":\n",
    "                scores, scores_rand = SVR_cv_plus_model_selection(\n",
    "                    cp[cp_features].values, l1k[l], k_fold, group_labels, 1\n",
    "                )\n",
    "            elif model == \"Ridge\":\n",
    "                scores, scores_rand = ridge_cv_plus_model_selection(\n",
    "                    cp[cp_features].values, l1k[l], k_fold, group_labels, 1\n",
    "                )\n",
    "            elif model == \"RFR\":\n",
    "                scores, scores_rand = RFR_cv_plus_model_selection(\n",
    "                    cp[cp_features].values, l1k[l], k_fold, group_labels, 1\n",
    "                )\n",
    "\n",
    "            pred_df[l] = scores\n",
    "            pred_df_rand[l] = scores_rand\n",
    "            gc.collect()\n",
    "            #             clear_output(wait=True)\n",
    "\n",
    "            print(\"time elapsed:\", (time.time() - start0) / 60)\n",
    "\n",
    "        ########################### mapping prob_ids to genes names\n",
    "        pred_df, _ = rename_affyprobe_to_genename(pred_df, l1k_features, \"../idmap.xlsx\")\n",
    "        pred_df_rand, _ = rename_affyprobe_to_genename(pred_df_rand, l1k_features, \"../idmap.xlsx\")\n",
    "\n",
    "        meltedPredDF = pd.melt(pred_df).rename(\n",
    "            columns={\"variable\": \"lmGens\", \"value\": \"pred score\"}\n",
    "        )\n",
    "        meltedPredDF_rand = pd.melt(pred_df_rand).rename(\n",
    "            columns={\"variable\": \"lmGens\", \"value\": \"pred score\"}\n",
    "        )\n",
    "        meltedPredDF[\"d\"] = \"n-folds\"\n",
    "        meltedPredDF_rand[\"d\"] = \"random\"\n",
    "        #         filename=results_dir+'/SingleGenePred/scores_hyperParam.xlsx'\n",
    "        filename = results_dir + \"/SingleGenePred/scores_randomSeed_fixed.xlsx\"\n",
    "\n",
    "        profTypeAbbrev = \"\".join([s[0] for s in profileType.split(\"_\")])\n",
    "\n",
    "#         saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "#                                      model+'-'+dataset+'-'+profTypeAbbrev+'-'+f+'-'+str(k_fold)+'-ht')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prediction of single CP features based on the full GE profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "# datasets=['LINCS', 'CDRP-bio'];\n",
    "datasets = [\"CDRP-bio\"]\n",
    "DT_kfold = {\"LUAD\": 9, \"TAORF\": 5, \"LINCS\": 25, \"CDRP-bio\": 6, \"CDRP\": 40}\n",
    "\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "# 'normalized_feature_select_dmso'\n",
    "profileType = \"normalized\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "# filter_perts='highRepOverlap'\n",
    "filter_perts = \"\"\n",
    "repCorrFilePath = \"../results/RepCor/RepCorrDF.xlsx\"\n",
    "\n",
    "filter_repCorr_params = [filter_perts, repCorrFilePath]\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "# def f(dataset):\n",
    "for dataset in datasets:\n",
    "    mergProf_treatLevel, cp_features, l1k_features = read_paired_treatment_level_profiles(\n",
    "        procProf_dir, dataset, profileType, filter_repCorr_params, 1\n",
    "    )\n",
    "\n",
    "    l1k = mergProf_treatLevel[[pertColName] + l1k_features]\n",
    "    cp = mergProf_treatLevel[[pertColName] + cp_features]\n",
    "\n",
    "    if dataset == \"LINCS\":\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"].str[0:13]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"].str[0:13]\n",
    "    else:\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels = le.fit_transform(l1k[\"Compounds\"].values)\n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled = l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values)\n",
    "    cp_scaled = cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(cp[cp_features].values.astype(\"float64\"))\n",
    "\n",
    "    for model in [\"MLP\"]:\n",
    "        #         if model==\"MLP\":\n",
    "        #             cp_scaled[cp_features] =preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit_transform(cp_scaled[cp_features].values)\n",
    "        #             cp_scaled[l1k_features] =preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit_transform(cp_scaled[l1k_features].values)\n",
    "\n",
    "        if 1:\n",
    "            cp = cp_scaled.copy()\n",
    "            l1k = l1k_scaled.copy()\n",
    "\n",
    "        ##############################\n",
    "\n",
    "        k_fold = DT_kfold[dataset]\n",
    "        #         k_fold=int(np.unique(group_labels).shape[0]/20)\n",
    "        pred_df = pd.DataFrame(index=range(k_fold), columns=cp_features)\n",
    "        pred_df_rand = pd.DataFrame(index=range(k_fold), columns=cp_features)\n",
    "        ii = 0\n",
    "        for c in cp_features:\n",
    "            start0 = time.time()\n",
    "            ii += 1\n",
    "            print(ii)\n",
    "            if model == \"Lasso\":\n",
    "                scores, scores_rand = lasso_cv_plus_model_selection(\n",
    "                    l1k[l1k_features], cp[c], k_fold, group_labels, 1\n",
    "                )\n",
    "            elif model == \"MLP\":\n",
    "                scores, scores_rand = MLP_cv_plus_model_selection_taorf_test(\n",
    "                    l1k[l1k_features].sample(frac=1), cp[c], k_fold, group_labels, 1\n",
    "                )\n",
    "            #                 scores,scores_rand=MLP_cv_plus_model_selection(l1k[l1k_features],cp[c],k_fold,group_labels,1)\n",
    "\n",
    "            #             sgsg\n",
    "            pred_df[c] = scores\n",
    "            pred_df_rand[c] = scores_rand\n",
    "\n",
    "            gc.collect()\n",
    "            print(\"time elapsed:\", (time.time() - start0) / 60)\n",
    "\n",
    "        meltedPredDF = pd.melt(pred_df).rename(\n",
    "            columns={\"variable\": \"CP-Features\", \"value\": \"pred score\"}\n",
    "        )\n",
    "        meltedPredDF_rand = pd.melt(pred_df_rand).rename(\n",
    "            columns={\"variable\": \"CP-Features\", \"value\": \"pred score\"}\n",
    "        )\n",
    "        meltedPredDF[\"d\"] = \"n-folds\"\n",
    "        meltedPredDF_rand[\"d\"] = \"random\"\n",
    "        #         filename='../../results/SingleCPfeatPred/scores.xlsx'\n",
    "        filename = results_dir + \"/SingleCPfeatPred/scores_randomSeed_fixed.xlsx\"\n",
    "\n",
    "        profTypeAbbrev = \"\".join([s[0] for s in profileType.split(\"_\")])\n",
    "\n",
    "        saveAsNewSheetToExistingFile(\n",
    "            filename,\n",
    "            pd.concat([meltedPredDF, meltedPredDF_rand], ignore_index=True),\n",
    "            model + \"-\" + dataset + \"-\" + profTypeAbbrev + \"-\" + f + \"-\" + str(k_fold) + \"-ht_2\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CP Category specific scores for single gene prediction - based on Lasso  regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "from warnings import simplefilter\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning(\"ignore\")\n",
    "import gc\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "# datasets=['LUAD', 'TAORF', 'LINCS', 'CDRP-bio'];\n",
    "datasets = [\"LUAD\"]\n",
    "\n",
    "# DT_kfold={'LUAD':10, 'TAORF':5, 'LINCS':20, 'CDRP-bio':20,'CDRP':40}\n",
    "\n",
    "DT_kfold = {\"LUAD\": 9, \"TAORF\": 5, \"LINCS\": 25, \"CDRP-bio\": 6, \"CDRP\": 40}\n",
    "\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "# 'normalized_feature_select_dmso'\n",
    "profileType = \"normalized\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"highRepOverlap\"\n",
    "repCorrFilePath = \"../results/RepCor/RepCorrDF.xlsx\"\n",
    "\n",
    "filter_repCorr_params = [filter_perts, repCorrFilePath]\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "profileLevel = \"treatment\"\n",
    "#'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "regr = MLPRegressor(\n",
    "    hidden_layer_sizes=(50, 10),\n",
    "    activation=\"logistic\",\n",
    "    alpha=0.7,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=50,\n",
    "    max_iter=1000,\n",
    ")\n",
    "\n",
    "\n",
    "# def f(dataset):\n",
    "for dataset in datasets:\n",
    "    mergProf_treatLevel, cp_features, l1k_features = read_paired_treatment_level_profiles(\n",
    "        procProf_dir, dataset, profileType, filter_repCorr_params, 1\n",
    "    )\n",
    "\n",
    "    l1k = mergProf_treatLevel[[pertColName] + l1k_features]\n",
    "    cp = mergProf_treatLevel[[pertColName] + cp_features]\n",
    "\n",
    "    if dataset == \"LINCS\":\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"].str[0:13]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"].str[0:13]\n",
    "    else:\n",
    "        cp[\"Compounds\"] = cp[\"PERT\"]\n",
    "        l1k[\"Compounds\"] = l1k[\"PERT\"]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    group_labels = le.fit_transform(l1k[\"Compounds\"].values)\n",
    "\n",
    "    scaler_ge = preprocessing.StandardScaler()\n",
    "    scaler_cp = preprocessing.StandardScaler()\n",
    "    l1k_scaled = l1k.copy()\n",
    "    l1k_scaled[l1k_features] = scaler_ge.fit_transform(l1k[l1k_features].values.astype(\"float64\"))\n",
    "    cp_scaled = cp.copy()\n",
    "    cp_scaled[cp_features] = scaler_cp.fit_transform(cp[cp_features].values.astype(\"float64\"))\n",
    "\n",
    "    if 1:\n",
    "        cp = cp_scaled.copy()\n",
    "        l1k = l1k_scaled.copy()\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    k_fold = DT_kfold[dataset]\n",
    "    split_obj = GroupKFold(n_splits=k_fold)\n",
    "\n",
    "    Channelss = [\"DNA\", \"RNA\", \"AGP\", \"Mito\", \"ER\"]\n",
    "    featureGroups = [\"Texture\", \"Intensity\", \"RadialDistribution\"]\n",
    "    relationMat_mpCat = pd.DataFrame(index=l1k_features)\n",
    "    for ch in range(len(Channelss)):\n",
    "        for f in range(len(featureGroups)):\n",
    "            clear_output(wait=True)\n",
    "            print(ch, f)\n",
    "\n",
    "            start0 = time.time()\n",
    "            selectedCols = cp.columns[\n",
    "                cp.columns.str.contains(Channelss[ch])\n",
    "                & cp.columns.str.contains(featureGroups[f])\n",
    "                & cp.columns.str.contains(\"Cells_|Cytoplasm_|Nuclei_\")\n",
    "            ].tolist()\n",
    "            if selectedCols != []:\n",
    "                ii = 0\n",
    "                for l in l1k_features:\n",
    "                    ii += 1\n",
    "                    print(ii)\n",
    "                    #                     start0 = time.time()\n",
    "                    scores, _ = MLP_cv_plus_model_selection_keras(\n",
    "                        cp[selectedCols].values, l1k[l], k_fold, group_labels, 0\n",
    "                    )\n",
    "                    #                     scores,scores_rand=lasso_cv_plus_model_selection(cp[selectedCols],l1k[l],k_fold,group_labels,0)\n",
    "                    #                     # Perform k-fold cross validation\n",
    "                    #                     scores = cross_val_score(regr, cp[selectedCols].values, l1k[l].values, groups=group_labels,cv=split_obj,n_jobs=k_fold)\n",
    "                    relationMat_mpCat.loc[l, Channelss[ch] + \"_\" + featureGroups[f]] = np.median(\n",
    "                        scores\n",
    "                    )\n",
    "                    gc.collect()\n",
    "                print(\"time elapsed:\", (time.time() - start0) / 60)\n",
    "\n",
    "    Channelss = [\"Nuclei\", \"Cytoplasm\", \"Cells\"]\n",
    "    featureGroups = [\"AreaShape\"]\n",
    "\n",
    "    for ch in range(len(Channelss)):\n",
    "        for f in range(len(featureGroups)):\n",
    "            clear_output(wait=True)\n",
    "            print(ch, f)\n",
    "            selectedCols = cp.columns[\n",
    "                cp.columns.str.contains(Channelss[ch])\n",
    "                & cp.columns.str.contains(featureGroups[f])\n",
    "                & cp.columns.str.contains(\"Cells_|Cytoplasm_|Nuclei_\")\n",
    "            ].tolist()\n",
    "            if selectedCols != []:\n",
    "                for l in l1k_features:\n",
    "                    scores, _ = MLP_cv_plus_model_selection_keras(\n",
    "                        cp[selectedCols].values, l1k[l], k_fold, group_labels, 0\n",
    "                    )\n",
    "                    #                     scores,scores_rand=lasso_cv_plus_model_selection(cp[selectedCols],l1k[l],k_fold,group_labels,0)\n",
    "                    #                     scores,scores_rand=MLP_cv_plus_model_selection(cp[selectedCols],l1k[l],k_fold,group_labels,0)\n",
    "                    #                     scores = cross_val_score(regr, cp[selectedCols].values, l1k[l].values, groups=group_labels,cv=split_obj,n_jobs=k_fold)\n",
    "                    relationMat_mpCat.loc[l, Channelss[ch] + \"_\" + featureGroups[f]] = np.median(\n",
    "                        scores\n",
    "                    )\n",
    "                    gc.collect()\n",
    "    ########################### mapping prob_ids to genes names\n",
    "    #     meta=pd.read_csv(\"/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/metadata/affy_probe_gene_mapping.txt\",delimiter=\"\\t\",header=None, names=[\"probe_id\", \"gene\"])\n",
    "    #     meta_gene_probID=meta.set_index('probe_id')\n",
    "    #     d = dict(zip(meta_gene_probID.index, meta_gene_probID['gene']))\n",
    "    #     relationMat_mpCat = relationMat_mpCat.rename(index=d)\n",
    "\n",
    "    filename = results_dir + \"/SingleGenePred_cpCategoryMap/cat_scores_maps.xlsx\"\n",
    "\n",
    "    profTypeAbbrev = \"\".join([s[0] for s in profileType.split(\"_\")])\n",
    "\n",
    "    saveAsNewSheetToExistingFile(\n",
    "        filename, relationMat_mpCat, dataset + \"-\" + str(k_fold) + \"-MLP-ht-corrected\"\n",
    "    )\n",
    "#     saveAsNewSheetToExistingFile(filename,relationMat_mpCat,dataset+'-'+str(k_fold)+'-MLP-keras-ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Leave dataset out Cross validation - on LUAD and LINCS (2-folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "datasets = [\"LUAD\", \"LINCS\"]\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType = \"normalized\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"highRepOverlap\"\n",
    "repCorrFilePath = \"../results/RepCor/RepCorrDF.xlsx\"\n",
    "\n",
    "filter_repCorr_params = [filter_perts, repCorrFilePath]\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "profileLevel = \"treatment\"\n",
    "#'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "\n",
    "mergProf_treatLevel_LI, cp_features_LI, l1k_features_LI = read_paired_treatment_level_profiles(\n",
    "    procProf_dir, \"LINCS\", profileType, filter_repCorr_params, 1\n",
    ")\n",
    "\n",
    "mergProf_treatLevel_LU, cp_features_LU, l1k_features_LU = read_paired_treatment_level_profiles(\n",
    "    procProf_dir, \"LUAD\", profileType, filter_repCorr_params, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(len(cp_features_LU),len(cp_features_LI),len(set(cp_features_LU) & set(cp_features_LI)))\n",
    "print(len(l1k_features_LU),len(l1k_features_LI),len(set(l1k_features_LU) & set(l1k_features_LI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cp_features_unionFs=list(set(cp_features_LU).union(set(cp_features_LI)))\n",
    "l1k_features_unionFs=list(set(l1k_features_LU).union(set(l1k_features_LI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(cp_features_unionFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cp_features_allOverlappingFs=list(set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs))\n",
    "l1k_features_allOverlappingFs=list(set(l1k_features_LU) & set(l1k_features_LI)& set(l1k_features_unionFs))\n",
    "\n",
    "len(cp_features_allOverlappingFs)\n",
    "# cp_features_LU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# cp_features=list(set(cp_features_LU) & set(cp_features_LI))\n",
    "# l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI))\n",
    "# profileType='normalized'\n",
    "\n",
    "cp_features=list(set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs))\n",
    "l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI)& set(l1k_features_unionFs))\n",
    "profileType='normalized_variable_selected_union'\n",
    "\n",
    "\n",
    "####################\n",
    "l1k_LU=mergProf_treatLevel_LU[[pertColName]+l1k_features]\n",
    "cp_LU=mergProf_treatLevel_LU[[pertColName]+cp_features]\n",
    "\n",
    "l1k_LI=mergProf_treatLevel_LI[[pertColName]+l1k_features]\n",
    "cp_LI=mergProf_treatLevel_LI[[pertColName]+cp_features]\n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "l1k_scaled_LU=l1k_LU.copy();l1k_scaled_LI=l1k_LI.copy()\n",
    "l1k_scaled_LU[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LU[l1k_features].values)\n",
    "l1k_scaled_LI[l1k_features] = preprocessing.StandardScaler().fit_transform(l1k_LI[l1k_features].values)\n",
    "\n",
    "cp_scaled_LU=cp_LU.copy();cp_scaled_LI=cp_LI.copy()\n",
    "cp_scaled_LU[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LU[cp_features].values.astype('float64'))\n",
    "cp_scaled_LI[cp_features] = preprocessing.StandardScaler().fit_transform(cp_LI[cp_features].values.astype('float64'))\n",
    "\n",
    "fold1=[[cp_scaled_LU,l1k_scaled_LU],[cp_scaled_LI,l1k_scaled_LI]]\n",
    "fold2=[[cp_scaled_LI,l1k_scaled_LI],[cp_scaled_LU,l1k_scaled_LU]]\n",
    "\n",
    "\n",
    "pred_df=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "pred_df_rand=pd.DataFrame(index=range(2),columns=l1k_features)\n",
    "\n",
    "# for dt in [fold1,fold2]:\n",
    "for n,dt in zip([0,1],[fold1,fold2]):\n",
    "    \n",
    "    for model in [\"Lasso\"]:#[\"Lasso\",\"MLP\",\"Ridge\"]:    \n",
    "\n",
    "        ##############################\n",
    "        ii=0\n",
    "        for l in l1k_features:\n",
    "            ii+=1\n",
    "            print(ii)\n",
    "\n",
    "            X_train, X_test = dt[0][0][cp_features].values, dt[1][0][cp_features].values\n",
    "            y_train, y_test = dt[0][1][l].values, dt[1][1][l]#.values\n",
    "\n",
    "            if model==\"Lasso\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas1 = np.linspace(0, 0.2, 20)\n",
    "                alphas2 = np.linspace(0.2, 5, 100)[1:]\n",
    "                alphas=np.concatenate((alphas1,alphas2))\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                lasso_cv = linear_model.LassoCV(alphas=alphas, random_state=0, max_iter=1000,selection='random')\n",
    "\n",
    "                lasso_cv.fit(X_train, y_train)  \n",
    "                r2_score=lasso_cv.score(X_test, y_test.values) \n",
    "                r2_rand=lasso_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "\n",
    "            if model==\"Ridge\":\n",
    "    #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)        \n",
    "    # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas = np.linspace(0.1, 0.2, 20)\n",
    "            #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                ridge_cv = linear_model.RidgeCV(alphas=alphas)\n",
    "\n",
    "                ridge_cv.fit(X_train, y_train)  \n",
    "                r2_score=ridge_cv.score(X_test, y_test.values) \n",
    "                r2_rand=ridge_cv.score(X_test, y_test.sample(frac = 1).values)\n",
    "                \n",
    "            elif model==\"MLP\":\n",
    "#                 scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "    #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)  \n",
    "                mlp_gs = MLPRegressor(activation='logistic',max_iter=500)\n",
    "                parameter_space = {\n",
    "                    'hidden_layer_sizes': [(50,),(10,30,10),(50,10),(50,10,10)],\n",
    "                    'alpha': [0.0001, 0.05,0.01,0.2],\n",
    "#                     'early_stopping':[True,False]\n",
    "                }\n",
    "\n",
    "                clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=2)\n",
    "                clf.fit(X_train, y_train)  \n",
    "                r2_score=clf.score(X_test, y_test.values)    \n",
    "                r2_rand=clf.score(X_test, y_test.sample(frac = 1).values)\n",
    "    \n",
    "            pred_df.loc[n,l]=r2_score\n",
    "            pred_df_rand.loc[n,l]=r2_rand\n",
    "\n",
    "    \n",
    "########################### mapping prob_ids to genes names    \n",
    "# meta=pd.read_csv(\"/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/metadata/affy_probe_gene_mapping.txt\",delimiter=\"\\t\",header=None, names=[\"probe_id\", \"gene\"])\n",
    "# meta_gene_probID=meta.set_index('probe_id')\n",
    "# d = dict(zip(meta_gene_probID.index, meta_gene_probID['gene']))\n",
    "# pred_df = pred_df.rename(columns=d)    \n",
    "# pred_df_rand = pred_df_rand.rename(columns=d)  \n",
    "\n",
    "pred_df,_=rename_affyprobe_to_genename(pred_df,l1k_features,'../idmap.xlsx')\n",
    "pred_df_rand,_=rename_affyprobe_to_genename(pred_df_rand,l1k_features,'../idmap.xlsx')\n",
    "\n",
    "pred_df.loc[0,'DT']='LUAD-LINCS'\n",
    "pred_df.loc[1,'DT']='LINCS-LUAD'\n",
    "\n",
    "pred_df_rand.loc[0,'DT']='LUAD-LINCS'\n",
    "pred_df_rand.loc[1,'DT']='LINCS-LUAD'\n",
    "\n",
    "meltedPredDF=pd.melt(pred_df,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF_rand=pd.melt(pred_df_rand,id_vars='DT').rename(columns={'variable':'lmGens','value':'pred score'})\n",
    "meltedPredDF['d']=\"n-folds\"\n",
    "meltedPredDF_rand['d']=\"random\"\n",
    "\n",
    "filename=results_dir+'/SingleGenePred/scores_cross_dts.xlsx'\n",
    "\n",
    "profTypeAbbrev=''.join([s[0] for s in profileType.split('_')])\n",
    "\n",
    "saveAsNewSheetToExistingFile(filename,pd.concat([meltedPredDF,meltedPredDF_rand],ignore_index=True),\\\n",
    "                             model+'-'+profTypeAbbrev+'-'+f+'-ht')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Leave dataset out Cross validation - on CDRP-bio and LINCS (2-folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# dataset options: 'CDRP' , 'LUAD', 'TAORF', 'LINCS', 'CDRP-bio'\n",
    "datasets = [\"CDRP-bio\", \"LINCS\"]\n",
    "# datasets=['LINCS', 'CDRP-bio','CDRP'];\n",
    "\n",
    "################################################\n",
    "# CP Profile Type options: 'augmented' , 'normalized', 'normalized_variable_selected'\n",
    "profileType = \"normalized\"\n",
    "\n",
    "################################################\n",
    "# filtering to compounds which have high replicates for both GE and CP datasets\n",
    "# highRepOverlapEnabled=0\n",
    "# 'highRepUnion','highRepOverlap'\n",
    "filter_perts = \"highRepOverlap\"\n",
    "\n",
    "################################################\n",
    "pertColName = \"PERT\"\n",
    "profileLevel = \"treatment\"\n",
    "#'replicate'  or  'treatment'\n",
    "if filter_perts:\n",
    "    f = \"filt\"\n",
    "else:\n",
    "    f = \"\"\n",
    "\n",
    "mergProf_treatLevel_LI, cp_features_LI, l1k_features_LI = read_paired_treatment_level_profiles(\n",
    "    procProf_dir, \"LINCS\", profileType, filter_perts, 1\n",
    ")\n",
    "\n",
    "mergProf_treatLevel_LU, cp_features_LU, l1k_features_LU = read_paired_treatment_level_profiles(\n",
    "    procProf_dir, \"CDRP-bio\", profileType, filter_perts, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cp_features_unionFs=list(set(cp_features_LU).union(set(cp_features_LI)))\n",
    "l1k_features_unionFs=list(set(l1k_features_LU).union(set(l1k_features_LI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# cp_features=list(set(cp_features_LU) & set(cp_features_LI))\n",
    "# l1k_features=list(set(l1k_features_LU) & set(l1k_features_LI))\n",
    "# profileType='normalized'\n",
    "\n",
    "cp_features = list(set(cp_features_LU) & set(cp_features_LI) & set(cp_features_unionFs))\n",
    "l1k_features = list(set(l1k_features_LU) & set(l1k_features_LI) & set(l1k_features_unionFs))\n",
    "profileType = \"normalized_variable_selected_union\"\n",
    "\n",
    "\n",
    "####################\n",
    "l1k_LU = mergProf_treatLevel_LU[[pertColName] + l1k_features]\n",
    "cp_LU = mergProf_treatLevel_LU[[pertColName] + cp_features]\n",
    "\n",
    "l1k_LI = mergProf_treatLevel_LI[[pertColName] + l1k_features]\n",
    "cp_LI = mergProf_treatLevel_LI[[pertColName] + cp_features]\n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "l1k_scaled_LU = l1k_LU.copy()\n",
    "l1k_scaled_LI = l1k_LI.copy()\n",
    "l1k_scaled_LU[l1k_features] = preprocessing.StandardScaler().fit_transform(\n",
    "    l1k_LU[l1k_features].values\n",
    ")\n",
    "l1k_scaled_LI[l1k_features] = preprocessing.StandardScaler().fit_transform(\n",
    "    l1k_LI[l1k_features].values\n",
    ")\n",
    "\n",
    "cp_scaled_LU = cp_LU.copy()\n",
    "cp_scaled_LI = cp_LI.copy()\n",
    "cp_scaled_LU[cp_features] = preprocessing.StandardScaler().fit_transform(\n",
    "    cp_LU[cp_features].values.astype(\"float64\")\n",
    ")\n",
    "cp_scaled_LI[cp_features] = preprocessing.StandardScaler().fit_transform(\n",
    "    cp_LI[cp_features].values.astype(\"float64\")\n",
    ")\n",
    "\n",
    "fold1 = [[cp_scaled_LU, l1k_scaled_LU], [cp_scaled_LI, l1k_scaled_LI]]\n",
    "fold2 = [[cp_scaled_LI, l1k_scaled_LI], [cp_scaled_LU, l1k_scaled_LU]]\n",
    "\n",
    "\n",
    "pred_df = pd.DataFrame(index=range(2), columns=l1k_features)\n",
    "pred_df_rand = pd.DataFrame(index=range(2), columns=l1k_features)\n",
    "\n",
    "# for dt in [fold1,fold2]:\n",
    "for n, dt in zip([0, 1], [fold1, fold2]):\n",
    "    for model in [\"Lasso\"]:  # [\"Lasso\",\"MLP\",\"Ridge\"]:\n",
    "        ##############################\n",
    "        ii = 0\n",
    "        for l in l1k_features:\n",
    "            ii += 1\n",
    "            print(ii)\n",
    "\n",
    "            X_train, X_test = dt[0][0][cp_features].values, dt[1][0][cp_features].values\n",
    "            y_train, y_test = dt[0][1][l].values, dt[1][1][l]  # .values\n",
    "\n",
    "            if model == \"Lasso\":\n",
    "                #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas1 = np.linspace(0, 0.2, 20)\n",
    "                alphas2 = np.linspace(0.2, 0.5, 10)[1:]\n",
    "                alphas = np.concatenate((alphas1, alphas2))\n",
    "                #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                lasso_cv = linear_model.LassoCV(\n",
    "                    alphas=alphas, random_state=0, max_iter=1000, selection=\"random\"\n",
    "                )\n",
    "\n",
    "                lasso_cv.fit(X_train, y_train)\n",
    "                r2_score = lasso_cv.score(X_test, y_test.values)\n",
    "                r2_rand = lasso_cv.score(X_test, y_test.sample(frac=1).values)\n",
    "\n",
    "            if model == \"Ridge\":\n",
    "                #             scores,scores_rand=lasso_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                # #                 scores,scores_rand=lasso_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                alphas = np.linspace(0.1, 0.2, 20)\n",
    "                #     alphas = np.logspace(-4, -0.5, 30)\n",
    "                ridge_cv = linear_model.RidgeCV(alphas=alphas)\n",
    "\n",
    "                ridge_cv.fit(X_train, y_train)\n",
    "                r2_score = ridge_cv.score(X_test, y_test.values)\n",
    "                r2_rand = ridge_cv.score(X_test, y_test.sample(frac=1).values)\n",
    "\n",
    "            elif model == \"MLP\":\n",
    "                #                 scores,scores_rand=MLP_cv_plus_model_selection(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                #                 scores,scores_rand=MLP_cv(cp[cp_features],l1k[l],k_fold,group_labels)\n",
    "                mlp_gs = MLPRegressor(activation=\"logistic\", max_iter=500)\n",
    "                parameter_space = {\n",
    "                    \"hidden_layer_sizes\": [(50,), (10, 30, 10), (50, 10), (50, 10, 10)],\n",
    "                    \"alpha\": [0.0001, 0.05, 0.01, 0.2],\n",
    "                    \"early_stopping\": [True, False],\n",
    "                }\n",
    "\n",
    "                clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=2)\n",
    "                clf.fit(X_train, y_train)\n",
    "                r2_score = clf.score(X_test, y_test.values)\n",
    "                r2_rand = clf.score(X_test, y_test.sample(frac=1).values)\n",
    "\n",
    "            pred_df.loc[n, l] = r2_score\n",
    "            pred_df_rand.loc[n, l] = r2_rand\n",
    "\n",
    "\n",
    "########################### mapping prob_ids to genes names\n",
    "meta = pd.read_csv(\n",
    "    \"/home/ubuntu/bucket/projects/2018_04_20_Rosetta/workspace/metadata/affy_probe_gene_mapping.txt\",\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"probe_id\", \"gene\"],\n",
    ")\n",
    "meta_gene_probID = meta.set_index(\"probe_id\")\n",
    "d = dict(zip(meta_gene_probID.index, meta_gene_probID[\"gene\"]))\n",
    "pred_df = pred_df.rename(columns=d)\n",
    "pred_df_rand = pred_df_rand.rename(columns=d)\n",
    "\n",
    "pred_df.loc[0, \"DT\"] = \"CDRPbio-LINCS\"\n",
    "pred_df.loc[1, \"DT\"] = \"LINCS-CDRPbio\"\n",
    "\n",
    "pred_df_rand.loc[0, \"DT\"] = \"CDRPbio-LINCS\"\n",
    "pred_df_rand.loc[1, \"DT\"] = \"LINCS-CDRPbio\"\n",
    "\n",
    "meltedPredDF = pd.melt(pred_df, id_vars=\"DT\").rename(\n",
    "    columns={\"variable\": \"lmGens\", \"value\": \"pred score\"}\n",
    ")\n",
    "meltedPredDF_rand = pd.melt(pred_df_rand, id_vars=\"DT\").rename(\n",
    "    columns={\"variable\": \"lmGens\", \"value\": \"pred score\"}\n",
    ")\n",
    "meltedPredDF[\"d\"] = \"n-folds\"\n",
    "meltedPredDF_rand[\"d\"] = \"random\"\n",
    "\n",
    "filename = results_dir + \"/SingleGenePred/scores_cross_dts_CD_LI.xlsx\"\n",
    "\n",
    "profTypeAbbrev = \"\".join([s[0] for s in profileType.split(\"_\")])\n",
    "\n",
    "saveAsNewSheetToExistingFile(\n",
    "    filename,\n",
    "    pd.concat([meltedPredDF, meltedPredDF_rand], ignore_index=True),\n",
    "    model + \"-\" + profTypeAbbrev + \"-\" + f + \"-ht\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model+'-'+profTypeAbbrev+'-'+f+'-ht'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mergProf_treatLevel_LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "438.212px",
    "left": "1507.78px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
